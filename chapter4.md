# ç¬¬4ç« ï¼šå¼ºåŒ–å­¦ä¹ ä¸RLHFæ·±åº¦è§£æ

ä»äººç±»åé¦ˆä¸­å­¦ä¹ ï¼ˆRLHFï¼‰å·²æˆä¸ºæ„å»ºå¯¹é½AIç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ã€‚æœ¬ç« æ·±å…¥æ¢è®¨RLHFçš„ç†è®ºåŸºç¡€ã€å®è·µç»†èŠ‚å’Œæœ€æ–°è¿›å±•ã€‚

## ç« èŠ‚ç›®å½•

1. [RLåŸºç¡€ä¸ç­–ç•¥æ¢¯åº¦æ–¹æ³•](#section1)
2. [RLHFçš„å®Œæ•´æµç¨‹å‰–æ](#section2)
3. [å¥–åŠ±æ¨¡å‹çš„è®¾è®¡ä¸è®­ç»ƒ](#section3)
4. [PPO vs DPO vs IPOç®—æ³•å¯¹æ¯”](#section4)
5. [Constitutional AIä¸è‡ªæˆ‘æ”¹è¿›](#section5)
6. [RLHFçš„æŒ‘æˆ˜ä¸æœªæ¥](#section6)

---

## <a name="section1"></a>4.1 RLåŸºç¡€ä¸ç­–ç•¥æ¢¯åº¦æ–¹æ³•

ç†è§£RLHFéœ€è¦å…ˆæŒæ¡å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µï¼Œç‰¹åˆ«æ˜¯ç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚

### 4.1.1 å¼ºåŒ–å­¦ä¹ åœ¨LLMä¸­çš„å½¢å¼åŒ–

**çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±çš„å®šä¹‰ï¼š**

åœ¨è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­ï¼š
- **çŠ¶æ€ï¼ˆStateï¼‰** $s_t$ï¼šå·²ç”Ÿæˆçš„tokenåºåˆ— $[x_1, x_2, ..., x_t]$
- **åŠ¨ä½œï¼ˆActionï¼‰** $a_t$ï¼šä¸‹ä¸€ä¸ªè¦ç”Ÿæˆçš„token
- **ç­–ç•¥ï¼ˆPolicyï¼‰** $\pi_\theta$ï¼šè¯­è¨€æ¨¡å‹æœ¬èº«ï¼Œ$P(a_t|s_t)$
- **å¥–åŠ±ï¼ˆRewardï¼‰** $r$ï¼šé€šå¸¸åªåœ¨åºåˆ—ç»“æŸæ—¶ç»™å‡º

**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼š**
```python
class LanguageGenerationMDP:
    def __init__(self, vocab_size, max_length):
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.eos_token = vocab_size - 1
    
    def step(self, state, action):
        # çŠ¶æ€è½¬ç§»æ˜¯ç¡®å®šæ€§çš„
        next_state = state + [action]
        
        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = (action == self.eos_token or 
                len(next_state) >= self.max_length)
        
        # å¥–åŠ±å»¶è¿Ÿåˆ°åºåˆ—ç»“æŸ
        reward = 0  # å°†ç”±å¥–åŠ±æ¨¡å‹è®¡ç®—
        
        return next_state, reward, done
```

### 4.1.2 ç­–ç•¥æ¢¯åº¦å®šç†

**ç›®æ ‡å‡½æ•°ï¼š**
$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]$$

å…¶ä¸­ $\tau = (s_0, a_0, s_1, a_1, ..., s_T)$ æ˜¯ä¸€ä¸ªè½¨è¿¹ã€‚

**ç­–ç•¥æ¢¯åº¦ï¼š**
$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot A_t\right]$$

å…¶ä¸­ $A_t$ æ˜¯ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantage Functionï¼‰ã€‚

**REINFORCEç®—æ³•å®ç°ï¼š**
```python
def reinforce_update(model, trajectories, optimizer):
    total_loss = 0
    
    for trajectory in trajectories:
        states, actions, rewards = trajectory
        
        # è®¡ç®—å›æŠ¥ï¼ˆä»åå‘å‰ï¼‰
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + gamma * G
            returns.insert(0, G)
        
        # å½’ä¸€åŒ–å›æŠ¥
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        # è®¡ç®—æŸå¤±
        for t, (state, action, G_t) in enumerate(zip(states, actions, returns)):
            log_prob = model.get_log_prob(state, action)
            loss = -log_prob * G_t
            total_loss += loss
    
    # æ›´æ–°å‚æ•°
    optimizer.zero_grad()
    (total_loss / len(trajectories)).backward()
    optimizer.step()
```

### 4.1.3 åŸºçº¿ä¸ä¼˜åŠ¿å‡½æ•°

**ä»·å€¼å‡½æ•°åŸºçº¿ï¼š**
```python
class ValueNetwork(nn.Module):
    def __init__(self, model_dim):
        super().__init__()
        self.value_head = nn.Sequential(
            nn.Linear(model_dim, model_dim),
            nn.ReLU(),
            nn.Linear(model_dim, 1)
        )
    
    def forward(self, hidden_states):
        # hidden_states: è¯­è¨€æ¨¡å‹çš„æœ€åéšè—çŠ¶æ€
        return self.value_head(hidden_states).squeeze(-1)
```

**å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰ï¼š**
$$A_t^{GAE} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}$$

å…¶ä¸­ $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ æ˜¯TDè¯¯å·®ã€‚

```python
def compute_gae(rewards, values, next_values, gamma=0.99, lam=0.95):
    advantages = []
    gae = 0
    
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = next_values
        else:
            next_value = values[t + 1]
        
        delta = rewards[t] + gamma * next_value - values[t]
        gae = delta + gamma * lam * gae
        advantages.insert(0, gae)
    
    return torch.tensor(advantages)
```

### 4.1.4 é‡è¦æ€§é‡‡æ ·ä¸PPO

**é‡è¦æ€§é‡‡æ ·æ¯”ç‡ï¼š**
$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$

**PPO-Clipç›®æ ‡ï¼š**
$$L^{CLIP}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t\right)\right]$$

**PPOå®ç°æ ¸å¿ƒï¼š**
```python
def ppo_loss(model, old_model, states, actions, advantages, epsilon=0.2):
    # è®¡ç®—æ–°æ—§ç­–ç•¥çš„logæ¦‚ç‡
    new_log_probs = model.get_log_probs(states, actions)
    old_log_probs = old_model.get_log_probs(states, actions).detach()
    
    # é‡è¦æ€§é‡‡æ ·æ¯”ç‡
    ratio = torch.exp(new_log_probs - old_log_probs)
    
    # PPO-ClipæŸå¤±
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages
    policy_loss = -torch.min(surr1, surr2).mean()
    
    # ä»·å€¼å‡½æ•°æŸå¤±
    values = model.get_values(states)
    value_targets = compute_returns(rewards, gamma)
    value_loss = F.mse_loss(values, value_targets)
    
    # ç†µæ­£åˆ™åŒ–ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
    entropy = model.get_entropy(states).mean()
    entropy_loss = -entropy_coef * entropy
    
    total_loss = policy_loss + value_coef * value_loss + entropy_loss
    
    return total_loss, {
        'policy_loss': policy_loss.item(),
        'value_loss': value_loss.item(),
        'entropy': entropy.item()
    }
```

### 4.1.5 KLæ•£åº¦çº¦æŸ

**ä¸ºä»€ä¹ˆéœ€è¦KLçº¦æŸï¼š**
é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§ï¼Œä¿æŒè®­ç»ƒç¨³å®šæ€§ã€‚

**KLæƒ©ç½šæ–¹æ³•ï¼š**
$$L(\theta) = \mathbb{E}_t[r_t(\theta)A_t] - \beta \cdot KL[\pi_\theta || \pi_{ref}]$$

```python
def compute_kl_penalty(model, ref_model, states):
    # è·å–æ•´ä¸ªè¯è¡¨ä¸Šçš„åˆ†å¸ƒ
    new_logits = model(states)
    ref_logits = ref_model(states).detach()
    
    # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    new_probs = F.softmax(new_logits, dim=-1)
    ref_probs = F.softmax(ref_logits, dim=-1)
    
    # KLæ•£åº¦
    kl = torch.sum(new_probs * (torch.log(new_probs) - torch.log(ref_probs)), dim=-1)
    
    return kl.mean()

class AdaptiveKLController:
    def __init__(self, init_kl_coef, target_kl):
        self.kl_coef = init_kl_coef
        self.target_kl = target_kl
    
    def update(self, current_kl):
        # è‡ªé€‚åº”è°ƒæ•´KLç³»æ•°
        if current_kl > 1.5 * self.target_kl:
            self.kl_coef *= 1.5
        elif current_kl < 0.5 * self.target_kl:
            self.kl_coef *= 0.5
        
        return self.kl_coef
```

#### ç»ƒä¹  4.1ï¼šå®ç°ç®€åŒ–ç‰ˆPPOè®­ç»ƒå¾ªç¯
å®ç°ä¸€ä¸ªç”¨äºè¯­è¨€æ¨¡å‹çš„PPOè®­ç»ƒå¾ªç¯ï¼ŒåŒ…æ‹¬é‡‡æ ·ã€ä¼˜åŠ¿è®¡ç®—å’Œå‚æ•°æ›´æ–°ã€‚

<details>
<summary>æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç®€åŒ–ç‰ˆPPOè®­ç»ƒå®ç°ï¼š**

```python
class SimplePPOTrainer:
    def __init__(self, policy_model, ref_model, reward_model, config):
        self.policy = policy_model
        self.ref_policy = ref_model
        self.reward_model = reward_model
        self.config = config
        
        # ä¼˜åŒ–å™¨
        self.optimizer = AdamW(
            self.policy.parameters(),
            lr=config.learning_rate
        )
        
        # ä»·å€¼ç½‘ç»œ
        self.value_net = ValueNetwork(policy_model.config.hidden_size)
        self.value_optimizer = AdamW(
            self.value_net.parameters(),
            lr=config.value_learning_rate
        )
        
        # KLæ§åˆ¶å™¨
        self.kl_controller = AdaptiveKLController(
            config.init_kl_coef,
            config.target_kl
        )
    
    def generate_trajectories(self, prompts, num_samples=1):
        """ç”Ÿæˆè½¨è¿¹å¹¶è®¡ç®—å¥–åŠ±"""
        trajectories = []
        
        for prompt in prompts:
            for _ in range(num_samples):
                # ç”Ÿæˆå“åº”
                with torch.no_grad():
                    response, log_probs = self.policy.generate_with_log_probs(
                        prompt,
                        max_length=self.config.max_length,
                        temperature=self.config.temperature
                    )
                
                # è®¡ç®—å¥–åŠ±
                reward = self.reward_model.compute_reward(prompt, response)
                
                # è®¡ç®—å‚è€ƒæ¨¡å‹çš„logæ¦‚ç‡ï¼ˆç”¨äºKLï¼‰
                ref_log_probs = self.ref_policy.get_log_probs(prompt, response)
                
                trajectories.append({
                    'prompt': prompt,
                    'response': response,
                    'log_probs': log_probs,
                    'ref_log_probs': ref_log_probs,
                    'reward': reward
                })
        
        return trajectories
    
    def compute_advantages(self, trajectories):
        """è®¡ç®—ä¼˜åŠ¿å‡½æ•°"""
        for traj in trajectories:
            states = self.get_states(traj['prompt'], traj['response'])
            
            # è®¡ç®—ä»·å€¼ä¼°è®¡
            with torch.no_grad():
                values = self.value_net(states)
            
            # è®¡ç®—GAE
            rewards = [0] * (len(states) - 1) + [traj['reward']]  # ç¨€ç–å¥–åŠ±
            advantages = compute_gae(
                rewards, 
                values,
                0,  # ç»ˆæ­¢çŠ¶æ€ä»·å€¼ä¸º0
                self.config.gamma,
                self.config.gae_lambda
            )
            
            traj['advantages'] = advantages
            traj['returns'] = values + advantages
    
    def update_policy(self, trajectories):
        """PPOç­–ç•¥æ›´æ–°"""
        # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
        states = []
        actions = []
        old_log_probs = []
        advantages = []
        returns = []
        
        for traj in trajectories:
            seq_states = self.get_states(traj['prompt'], traj['response'])
            seq_actions = self.get_actions(traj['response'])
            
            states.extend(seq_states[:-1])  # é™¤äº†æœ€åä¸€ä¸ªçŠ¶æ€
            actions.extend(seq_actions)
            old_log_probs.extend(traj['log_probs'])
            advantages.extend(traj['advantages'])
            returns.extend(traj['returns'])
        
        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.stack(states)
        actions = torch.tensor(actions)
        old_log_probs = torch.tensor(old_log_probs)
        advantages = torch.tensor(advantages)
        returns = torch.tensor(returns)
        
        # å½’ä¸€åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPOæ›´æ–°å¾ªç¯
        for _ in range(self.config.ppo_epochs):
            # éšæœºæ‰“ä¹±
            indices = torch.randperm(len(states))
            
            for start in range(0, len(states), self.config.batch_size):
                end = start + self.config.batch_size
                batch_indices = indices[start:end]
                
                # è·å–æ‰¹æ¬¡
                b_states = states[batch_indices]
                b_actions = actions[batch_indices]
                b_old_log_probs = old_log_probs[batch_indices]
                b_advantages = advantages[batch_indices]
                b_returns = returns[batch_indices]
                
                # è®¡ç®—æ–°çš„logæ¦‚ç‡
                new_log_probs = self.policy.get_log_probs(b_states, b_actions)
                
                # é‡è¦æ€§é‡‡æ ·æ¯”ç‡
                ratio = torch.exp(new_log_probs - b_old_log_probs)
                
                # PPOæŸå¤±
                surr1 = ratio * b_advantages
                surr2 = torch.clamp(
                    ratio, 
                    1 - self.config.clip_epsilon,
                    1 + self.config.clip_epsilon
                ) * b_advantages
                
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # ä»·å€¼æŸå¤±
                values = self.value_net(b_states)
                value_loss = F.mse_loss(values, b_returns)
                
                # KLæƒ©ç½š
                kl_penalty = compute_kl_penalty(
                    self.policy,
                    self.ref_policy,
                    b_states
                )
                
                # æ€»æŸå¤±
                loss = (policy_loss + 
                       self.config.value_coef * value_loss +
                       self.kl_controller.kl_coef * kl_penalty)
                
                # æ›´æ–°
                self.optimizer.zero_grad()
                self.value_optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    self.policy.parameters(),
                    self.config.max_grad_norm
                )
                
                self.optimizer.step()
                self.value_optimizer.step()
        
        # æ›´æ–°KLæ§åˆ¶å™¨
        with torch.no_grad():
            current_kl = compute_kl_penalty(
                self.policy,
                self.ref_policy,
                states
            ).item()
        
        self.kl_controller.update(current_kl)
    
    def train_step(self, prompts):
        """å•æ­¥è®­ç»ƒ"""
        # 1. ç”Ÿæˆè½¨è¿¹
        trajectories = self.generate_trajectories(prompts)
        
        # 2. è®¡ç®—ä¼˜åŠ¿
        self.compute_advantages(trajectories)
        
        # 3. æ›´æ–°ç­–ç•¥
        self.update_policy(trajectories)
        
        # 4. è®°å½•æŒ‡æ ‡
        metrics = self.compute_metrics(trajectories)
        
        return metrics
    
    def compute_metrics(self, trajectories):
        """è®¡ç®—è®­ç»ƒæŒ‡æ ‡"""
        rewards = [traj['reward'] for traj in trajectories]
        kl_divs = []
        
        for traj in trajectories:
            kl = torch.sum(
                torch.exp(traj['log_probs']) * 
                (traj['log_probs'] - traj['ref_log_probs'])
            ).item()
            kl_divs.append(kl)
        
        return {
            'mean_reward': np.mean(rewards),
            'std_reward': np.std(rewards),
            'mean_kl': np.mean(kl_divs),
            'kl_coef': self.kl_controller.kl_coef
        }
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
# é…ç½®
config = PPOConfig(
    learning_rate=1e-6,
    value_learning_rate=1e-5,
    batch_size=128,
    ppo_epochs=4,
    clip_epsilon=0.2,
    value_coef=0.1,
    init_kl_coef=0.1,
    target_kl=0.01,
    gamma=0.99,
    gae_lambda=0.95,
    max_grad_norm=1.0,
    temperature=0.7,
    max_length=512
)

# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = SimplePPOTrainer(
    policy_model=policy_model,
    ref_model=ref_model,
    reward_model=reward_model,
    config=config
)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    # è·å–è®­ç»ƒprompts
    prompts = get_training_prompts(batch_size=32)
    
    # è®­ç»ƒæ­¥éª¤
    metrics = trainer.train_step(prompts)
    
    # è®°å½•æ—¥å¿—
    print(f"Epoch {epoch}: {metrics}")
    
    # å®šæœŸè¯„ä¼°
    if epoch % eval_freq == 0:
        eval_metrics = evaluate_model(trainer.policy)
        print(f"Evaluation: {eval_metrics}")
```

</details>

### 4.1.6 ç­–ç•¥æ¢¯åº¦çš„æ–¹å·®ç¼©å‡æŠ€æœ¯

**æ§åˆ¶å˜é‡æ³•ï¼š**
```python
def compute_advantages_with_baseline(rewards, values, gamma=0.99):
    advantages = []
    returns = []
    
    # è®¡ç®—çœŸå®å›æŠ¥
    G = 0
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)
    
    # è®¡ç®—ä¼˜åŠ¿ï¼ˆå›æŠ¥ - åŸºçº¿ï¼‰
    for ret, val in zip(returns, values):
        advantages.append(ret - val)
    
    return torch.tensor(advantages), torch.tensor(returns)
```

**å½’ä¸€åŒ–æŠ€å·§ï¼š**
```python
class RunningMeanStd:
    """è¿è¡Œæ—¶å‡å€¼å’Œæ ‡å‡†å·®ä¼°è®¡"""
    def __init__(self, shape=()):
        self.mean = np.zeros(shape, dtype=np.float64)
        self.var = np.ones(shape, dtype=np.float64)
        self.count = 1e-8
    
    def update(self, x):
        batch_mean = np.mean(x, axis=0)
        batch_var = np.var(x, axis=0)
        batch_count = x.shape[0]
        
        delta = batch_mean - self.mean
        self.mean += delta * batch_count / (self.count + batch_count)
        
        m_a = self.var * self.count
        m_b = batch_var * batch_count
        M2 = m_a + m_b + delta**2 * self.count * batch_count / (self.count + batch_count)
        
        self.var = M2 / (self.count + batch_count)
        self.count += batch_count
    
    def normalize(self, x):
        return (x - self.mean) / (np.sqrt(self.var) + 1e-8)
```

**âš¡ è®¾è®¡é€‰æ‹©ï¼š**
PPOåœ¨è¯­è¨€æ¨¡å‹ä¸­çš„è®¾è®¡æƒè¡¡ï¼š
- ClipèŒƒå›´ï¼šå¤ªå°é™åˆ¶å­¦ä¹ ï¼Œå¤ªå¤§å¤±å»ç¨³å®šæ€§
- KLæƒ©ç½šï¼šç¡¬çº¦æŸvsè½¯çº¦æŸ
- ä»·å€¼å‡½æ•°ï¼šç‹¬ç«‹ç½‘ç»œvså…±äº«ä¸»å¹²
- æ‰¹æ¬¡å¤§å°ï¼šå¤§æ‰¹æ¬¡ç¨³å®šä½†è®¡ç®—æ˜‚è´µ

**ğŸ”¬ ç ”ç©¶çº¿ç´¢ï¼š**
- å¦‚ä½•æ›´å¥½åœ°å¤„ç†ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Ÿ
- ç¦»çº¿RLåœ¨RLHFä¸­çš„åº”ç”¨ï¼Ÿ
- å¤šç›®æ ‡RLå¦‚ä½•å¹³è¡¡ä¸åŒå¯¹é½ç›®æ ‡ï¼Ÿ

---

## <a name="section2"></a>4.2 RLHFçš„å®Œæ•´æµç¨‹å‰–æ

RLHFä¸æ˜¯å•ä¸€æŠ€æœ¯ï¼Œè€Œæ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªé˜¶æ®µçš„å¤æ‚æµç¨‹ã€‚æœ¬èŠ‚è¯¦ç»†å‰–ææ¯ä¸ªé˜¶æ®µçš„è®¾è®¡ä¸å®ç°ã€‚

### 4.2.1 RLHFä¸‰é˜¶æ®µæ¦‚è§ˆ

**å®Œæ•´æµç¨‹ï¼š**

```mermaid
graph LR
    A[é¢„è®­ç»ƒæ¨¡å‹] --> B[SFTé˜¶æ®µ]
    B --> C[å¥–åŠ±æ¨¡å‹è®­ç»ƒ]
    C --> D[PPOå¾®è°ƒ]
    D --> E[å¯¹é½çš„æ¨¡å‹]
```

**å„é˜¶æ®µçš„ä½œç”¨ï¼š**
1. **SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰**ï¼šå»ºç«‹åŸºç¡€èƒ½åŠ›
2. **å¥–åŠ±å»ºæ¨¡**ï¼šå­¦ä¹ äººç±»åå¥½
3. **RLå¾®è°ƒ**ï¼šä¼˜åŒ–åå¥½å¯¹é½

### 4.2.2 é˜¶æ®µ1ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦SFTï¼š**
- è®©æ¨¡å‹å­¦ä¼šåŸºæœ¬çš„æŒ‡ä»¤éµå¾ªæ ¼å¼
- æä¾›è‰¯å¥½çš„åˆå§‹ç­–ç•¥
- å‡å°‘RLé˜¶æ®µçš„æ¢ç´¢éš¾åº¦

**SFTæ•°æ®æ„å»ºï¼š**
```python
def prepare_sft_data(raw_demonstrations):
    sft_examples = []
    
    for demo in raw_demonstrations:
        # æ ‡å‡†åŒ–æ ¼å¼
        formatted_input = format_prompt(demo['instruction'])
        formatted_output = clean_response(demo['response'])
        
        # è´¨é‡è¿‡æ»¤
        if is_high_quality(formatted_output):
            sft_examples.append({
                'input': formatted_input,
                'output': formatted_output,
                'metadata': {
                    'source': demo.get('source'),
                    'quality_score': rate_quality(formatted_output)
                }
            })
    
    # å»é‡
    sft_examples = deduplicate(sft_examples)
    
    # å¹³è¡¡æ•°æ®åˆ†å¸ƒ
    sft_examples = balance_categories(sft_examples)
    
    return sft_examples
```

**SFTè®­ç»ƒç­–ç•¥ï¼š**
```python
class SFTTrainer:
    def __init__(self, model, config):
        self.model = model
        self.config = config
        
    def train_step(self, batch):
        inputs = self.tokenizer(
            batch['inputs'],
            padding=True,
            truncation=True,
            return_tensors='pt'
        )
        
        targets = self.tokenizer(
            batch['outputs'],
            padding=True,
            truncation=True,
            return_tensors='pt'
        )
        
        # åªè®¡ç®—è¾“å‡ºéƒ¨åˆ†çš„æŸå¤±
        input_len = inputs['input_ids'].shape[1]
        labels = torch.full_like(inputs['input_ids'], -100)
        labels[:, input_len:] = targets['input_ids']
        
        outputs = self.model(
            input_ids=inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            labels=labels
        )
        
        return outputs.loss
```

### 4.2.3 é˜¶æ®µ2ï¼šåå¥½æ•°æ®æ”¶é›†

**åå¥½æ•°æ®çš„ç±»å‹ï¼š**

**1. æˆå¯¹æ¯”è¾ƒï¼š**
```python
class PairwiseComparison:
    def __init__(self, prompt, response_a, response_b, preference):
        self.prompt = prompt
        self.responses = [response_a, response_b]
        self.preference = preference  # 0, 0.5, or 1
        
    def to_training_pair(self):
        if self.preference == 0.5:
            # å¹³å±€å¤„ç†
            return None
        elif self.preference == 1:
            return {
                'chosen': self.responses[0],
                'rejected': self.responses[1],
                'prompt': self.prompt
            }
        else:
            return {
                'chosen': self.responses[1],
                'rejected': self.responses[0],
                'prompt': self.prompt
            }
```

**2. è¯„åˆ†æ•°æ®ï¼š**
```python
def convert_ratings_to_comparisons(rated_responses):
    """å°†è¯„åˆ†è½¬æ¢ä¸ºæˆå¯¹æ¯”è¾ƒ"""
    comparisons = []
    
    # æŒ‰promptåˆ†ç»„
    grouped = defaultdict(list)
    for item in rated_responses:
        grouped[item['prompt']].append(item)
    
    # ç”Ÿæˆæ¯”è¾ƒå¯¹
    for prompt, responses in grouped.items():
        # æŒ‰åˆ†æ•°æ’åº
        responses.sort(key=lambda x: x['rating'], reverse=True)
        
        # åˆ›å»ºæ¯”è¾ƒå¯¹ï¼ˆé¿å…åˆ†æ•°å¤ªæ¥è¿‘çš„ï¼‰
        for i in range(len(responses)):
            for j in range(i + 1, len(responses)):
                if responses[i]['rating'] - responses[j]['rating'] > 0.5:
                    comparisons.append({
                        'prompt': prompt,
                        'chosen': responses[i]['response'],
                        'rejected': responses[j]['response'],
                        'margin': responses[i]['rating'] - responses[j]['rating']
                    })
    
    return comparisons
```

**åå¥½æ•°æ®è´¨é‡æ§åˆ¶ï¼š**
```python
class PreferenceDataValidator:
    def __init__(self):
        self.annotator_agreement_threshold = 0.7
        self.min_margin_threshold = 0.2
    
    def validate_comparison(self, comparison_data):
        # æ£€æŸ¥æ ‡æ³¨è€…ä¸€è‡´æ€§
        if 'annotator_scores' in comparison_data:
            agreement = self.compute_agreement(comparison_data['annotator_scores'])
            if agreement < self.annotator_agreement_threshold:
                return False, "Low annotator agreement"
        
        # æ£€æŸ¥åå¥½å¼ºåº¦
        if comparison_data.get('margin', 1.0) < self.min_margin_threshold:
            return False, "Preference margin too small"
        
        # æ£€æŸ¥å“åº”è´¨é‡
        if self.is_degenerate(comparison_data['chosen']):
            return False, "Chosen response is degenerate"
        
        return True, "Valid"
    
    def compute_agreement(self, scores):
        # è®¡ç®—Krippendorff's alphaæˆ–ç±»ä¼¼æŒ‡æ ‡
        # ç®€åŒ–ç‰ˆï¼šè®¡ç®—æ ‡å‡†å·®
        return 1 - np.std(scores) / np.mean(scores)
```

### 4.2.4 é˜¶æ®µ3ï¼šå¥–åŠ±æ¨¡å‹è®­ç»ƒ

**å¥–åŠ±æ¨¡å‹æ¶æ„ï¼š**
```python
class RewardModel(nn.Module):
    def __init__(self, base_model, config):
        super().__init__()
        self.base_model = base_model
        self.config = config
        
        # å¥–åŠ±å¤´
        self.reward_head = nn.Linear(
            base_model.config.hidden_size,
            1,
            bias=False
        )
        
        # åˆå§‹åŒ–
        nn.init.normal_(self.reward_head.weight, std=1e-3)
    
    def forward(self, input_ids, attention_mask=None):
        # è·å–åŸºç¡€æ¨¡å‹è¾“å‡º
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªtokençš„éšè—çŠ¶æ€
        hidden_states = outputs.hidden_states[-1]
        last_hidden = hidden_states[:, -1, :]
        
        # è®¡ç®—å¥–åŠ±åˆ†æ•°
        reward = self.reward_head(last_hidden)
        
        return reward
```

**å¥–åŠ±æ¨¡å‹æŸå¤±å‡½æ•°ï¼š**
```python
def reward_model_loss(reward_model, batch):
    # è·å–chosenå’Œrejectedçš„å¥–åŠ±
    chosen_rewards = reward_model(
        batch['chosen_input_ids'],
        batch['chosen_attention_mask']
    )
    
    rejected_rewards = reward_model(
        batch['rejected_input_ids'],
        batch['rejected_attention_mask']
    )
    
    # Bradley-Terryæ¨¡å‹æŸå¤±
    # P(chosen > rejected) = sigmoid(r_chosen - r_rejected)
    logits = chosen_rewards - rejected_rewards
    
    # å¦‚æœæœ‰åå¥½å¼ºåº¦ï¼Œå¯ä»¥åŠ æƒ
    if 'preference_margin' in batch:
        # å¼ºåå¥½çš„æƒé‡æ›´é«˜
        weights = batch['preference_margin']
        loss = -F.logsigmoid(logits * weights).mean()
    else:
        loss = -F.logsigmoid(logits).mean()
    
    # å‡†ç¡®ç‡
    with torch.no_grad():
        accuracy = (logits > 0).float().mean()
    
    return loss, accuracy
```

### 4.2.5 é˜¶æ®µ4ï¼šPPOè®­ç»ƒ

**å®Œæ•´PPOæµç¨‹æ•´åˆï¼š**
```python
class RLHFPPOTrainer:
    def __init__(self, policy_model, ref_model, reward_model, config):
        self.policy = policy_model
        self.ref_policy = ref_model
        self.reward_model = reward_model
        self.config = config
        
        # é‡‡æ ·å‚æ•°
        self.generation_config = GenerationConfig(
            max_length=config.max_length,
            temperature=config.temperature,
            top_p=config.top_p,
            do_sample=True
        )
    
    def make_experience(self, prompts):
        """ç”Ÿæˆç»éªŒæ•°æ®"""
        experiences = []
        
        with torch.no_grad():
            for prompt in prompts:
                # ç”Ÿæˆå“åº”
                prompt_ids = self.tokenizer.encode(prompt, return_tensors='pt')
                response_ids = self.policy.generate(
                    prompt_ids,
                    **self.generation_config.__dict__
                )
                
                # åªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†
                response_ids = response_ids[:, prompt_ids.shape[1]:]
                
                # è®¡ç®—å¥–åŠ±
                full_ids = torch.cat([prompt_ids, response_ids], dim=1)
                reward_score = self.reward_model(full_ids).item()
                
                # è®¡ç®—KLæƒ©ç½š
                policy_logits = self.policy(full_ids).logits
                ref_logits = self.ref_policy(full_ids).logits
                
                kl_penalty = compute_sequence_kl(
                    policy_logits[:, prompt_ids.shape[1]-1:-1],
                    ref_logits[:, prompt_ids.shape[1]-1:-1],
                    response_ids
                )
                
                # ç»„åˆå¥–åŠ±
                total_reward = reward_score - self.config.kl_coef * kl_penalty
                
                experiences.append({
                    'prompt': prompt,
                    'prompt_ids': prompt_ids,
                    'response_ids': response_ids,
                    'reward': total_reward,
                    'reward_score': reward_score,
                    'kl_penalty': kl_penalty
                })
        
        return experiences
    
    def train_minibatch(self, experiences):
        """è®­ç»ƒå°æ‰¹æ¬¡"""
        # å‡†å¤‡æ•°æ®
        all_input_ids = []
        all_rewards = []
        all_advantages = []
        all_returns = []
        old_log_probs = []
        
        for exp in experiences:
            # è®¡ç®—æ¯ä¸ªtokençš„logæ¦‚ç‡
            full_ids = torch.cat([exp['prompt_ids'], exp['response_ids']], dim=1)
            
            with torch.no_grad():
                logits = self.policy(full_ids).logits
                log_probs = gather_log_probs(logits, full_ids)
                
                # åªä¿ç•™å“åº”éƒ¨åˆ†çš„log_probs
                response_log_probs = log_probs[:, exp['prompt_ids'].shape[1]:]
                
                # è®¡ç®—ä»·å€¼ä¼°è®¡
                values = self.value_head(self.policy.get_hidden_states(full_ids))
                response_values = values[:, exp['prompt_ids'].shape[1]:]
            
            # è®¡ç®—ä¼˜åŠ¿
            rewards = [0] * (len(response_log_probs[0]) - 1) + [exp['reward']]
            advantages, returns = compute_advantages_and_returns(
                rewards,
                response_values[0].tolist(),
                self.config.gamma,
                self.config.lam
            )
            
            all_input_ids.append(full_ids)
            all_rewards.append(exp['reward'])
            all_advantages.extend(advantages)
            all_returns.extend(returns)
            old_log_probs.extend(response_log_probs[0].tolist())
        
        # PPOæ›´æ–°
        for _ in range(self.config.ppo_epochs):
            # è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
            logits = self.policy(torch.cat(all_input_ids, dim=0)).logits
            current_log_probs = gather_log_probs(logits, torch.cat(all_input_ids, dim=0))
            
            # åªå–å“åº”éƒ¨åˆ†
            response_log_probs = []
            idx = 0
            for exp in experiences:
                prompt_len = exp['prompt_ids'].shape[1]
                response_len = exp['response_ids'].shape[1]
                response_log_probs.extend(
                    current_log_probs[idx, prompt_len:prompt_len+response_len]
                )
                idx += 1
            
            # è®¡ç®—æ¯”ç‡
            ratio = torch.exp(
                torch.tensor(response_log_probs) - torch.tensor(old_log_probs)
            )
            
            # PPOæŸå¤±
            advantages_tensor = torch.tensor(all_advantages)
            clipped_ratio = torch.clamp(ratio, 1-self.config.clip_range, 1+self.config.clip_range)
            
            policy_loss = -torch.min(
                ratio * advantages_tensor,
                clipped_ratio * advantages_tensor
            ).mean()
            
            # ä»·å€¼æŸå¤±
            current_values = self.value_head(
                self.policy.get_hidden_states(torch.cat(all_input_ids, dim=0))
            )
            value_loss = F.mse_loss(
                current_values.squeeze(),
                torch.tensor(all_returns)
            )
            
            # æ€»æŸå¤±
            loss = policy_loss + self.config.vf_coef * value_loss
            
            # ä¼˜åŒ–
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                self.policy.parameters(),
                self.config.max_grad_norm
            )
            self.optimizer.step()
        
        # è¿”å›ç»Ÿè®¡ä¿¡æ¯
        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'mean_reward': np.mean(all_rewards),
            'mean_kl': np.mean([exp['kl_penalty'] for exp in experiences])
        }
```

#### ç»ƒä¹  4.2ï¼šå®ç°RLHFæ•°æ®æ”¶é›†pipeline
è®¾è®¡å¹¶å®ç°ä¸€ä¸ªå®Œæ•´çš„RLHFæ•°æ®æ”¶é›†ç³»ç»Ÿï¼ŒåŒ…æ‹¬å“åº”ç”Ÿæˆã€äººç±»æ ‡æ³¨æ¥å£å’Œè´¨é‡æ§åˆ¶ã€‚

<details>
<summary>æŸ¥çœ‹ç­”æ¡ˆ</summary>

**RLHFæ•°æ®æ”¶é›†ç³»ç»Ÿå®ç°ï¼š**

```python
class RLHFDataCollectionPipeline:
    def __init__(self, models, annotator_pool, config):
        self.models = models  # ç”¨äºç”Ÿæˆä¸åŒå“åº”çš„æ¨¡å‹åˆ—è¡¨
        self.annotator_pool = annotator_pool
        self.config = config
        
        # æ•°æ®å­˜å‚¨
        self.prompt_bank = PromptBank()
        self.response_cache = ResponseCache()
        self.preference_database = PreferenceDatabase()
        
        # è´¨é‡æ§åˆ¶
        self.quality_controller = QualityController(config.quality_thresholds)
    
    def collect_preferences(self, num_comparisons):
        """ä¸»æ•°æ®æ”¶é›†å¾ªç¯"""
        collected = 0
        
        while collected < num_comparisons:
            # 1. é€‰æ‹©prompts
            prompts = self.select_prompts(
                batch_size=self.config.batch_size
            )
            
            # 2. ç”Ÿæˆå“åº”
            response_sets = self.generate_diverse_responses(prompts)
            
            # 3. åˆ›å»ºæ¯”è¾ƒä»»åŠ¡
            comparison_tasks = self.create_comparison_tasks(
                prompts,
                response_sets
            )
            
            # 4. åˆ†é…ç»™æ ‡æ³¨è€…
            annotations = self.distribute_to_annotators(comparison_tasks)
            
            # 5. è´¨é‡éªŒè¯
            valid_annotations = self.validate_annotations(annotations)
            
            # 6. å­˜å‚¨ç»“æœ
            self.store_preferences(valid_annotations)
            
            collected += len(valid_annotations)
            
            # 7. æ›´æ–°ç»Ÿè®¡
            self.update_statistics(valid_annotations)
        
        return self.preference_database.get_all()
    
    def select_prompts(self, batch_size):
        """æ™ºèƒ½prompté€‰æ‹©"""
        # ç»“åˆå¤šç§ç­–ç•¥
        prompts = []
        
        # 40% ä»é«˜ä»·å€¼promptæ± 
        high_value = self.prompt_bank.get_high_value_prompts(
            int(batch_size * 0.4)
        )
        prompts.extend(high_value)
        
        # 30% è¦†ç›–ä¸è¶³çš„ç±»åˆ«
        underrepresented = self.prompt_bank.get_underrepresented_categories(
            int(batch_size * 0.3)
        )
        prompts.extend(underrepresented)
        
        # 20% å¯¹æŠ—æ€§/è¾¹ç•Œæ¡ˆä¾‹
        adversarial = self.prompt_bank.get_adversarial_prompts(
            int(batch_size * 0.2)
        )
        prompts.extend(adversarial)
        
        # 10% éšæœºæ¢ç´¢
        random_prompts = self.prompt_bank.get_random_prompts(
            batch_size - len(prompts)
        )
        prompts.extend(random_prompts)
        
        return prompts
    
    def generate_diverse_responses(self, prompts):
        """ç”Ÿæˆå¤šæ ·åŒ–å“åº”"""
        response_sets = defaultdict(list)
        
        for prompt in prompts:
            # æ£€æŸ¥ç¼“å­˜
            cached = self.response_cache.get(prompt)
            if cached and len(cached) >= self.config.responses_per_prompt:
                response_sets[prompt] = cached
                continue
            
            responses = []
            
            # ä½¿ç”¨ä¸åŒæ¨¡å‹ç”Ÿæˆ
            for model in self.models[:self.config.models_per_prompt]:
                response = self.generate_with_model(model, prompt)
                responses.append({
                    'text': response,
                    'model': model.name,
                    'params': model.generation_params
                })
            
            # ä½¿ç”¨åŒä¸€æ¨¡å‹çš„ä¸åŒå‚æ•°
            main_model = self.models[0]
            for temp in [0.7, 0.9, 1.1]:
                response = self.generate_with_model(
                    main_model,
                    prompt,
                    temperature=temp
                )
                responses.append({
                    'text': response,
                    'model': main_model.name,
                    'params': {'temperature': temp}
                })
            
            # å»é‡
            responses = self.deduplicate_responses(responses)
            
            # ç¡®ä¿å¤šæ ·æ€§
            if len(responses) < self.config.min_responses:
                responses.extend(
                    self.generate_forced_diversity(
                        main_model,
                        prompt,
                        existing=responses,
                        needed=self.config.min_responses - len(responses)
                    )
                )
            
            response_sets[prompt] = responses
            self.response_cache.add(prompt, responses)
        
        return response_sets
    
    def create_comparison_tasks(self, prompts, response_sets):
        """åˆ›å»ºæ¯”è¾ƒä»»åŠ¡"""
        tasks = []
        
        for prompt in prompts:
            responses = response_sets[prompt]
            
            # é€‰æ‹©æ¯”è¾ƒç­–ç•¥
            if len(responses) <= 3:
                # å…¨éƒ¨ä¸¤ä¸¤æ¯”è¾ƒ
                pairs = list(itertools.combinations(responses, 2))
            else:
                # æ™ºèƒ½é‡‡æ ·
                pairs = self.smart_pair_sampling(responses)
            
            for resp1, resp2 in pairs:
                task = ComparisonTask(
                    prompt=prompt,
                    response_a=resp1,
                    response_b=resp2,
                    metadata={
                        'created_at': datetime.now(),
                        'sampling_strategy': 'smart' if len(responses) > 3 else 'exhaustive'
                    }
                )
                tasks.append(task)
        
        return tasks
    
    def smart_pair_sampling(self, responses):
        """æ™ºèƒ½é…å¯¹é‡‡æ ·"""
        pairs = []
        
        # 1. ç¡®ä¿æ¯ä¸ªå“åº”è‡³å°‘è¢«æ¯”è¾ƒä¸€æ¬¡
        response_coverage = {i: 0 for i in range(len(responses))}
        
        # 2. ä¼˜å…ˆæ¯”è¾ƒä¸åŒæ¨¡å‹/å‚æ•°çš„å“åº”
        different_source_pairs = []
        for i, resp1 in enumerate(responses):
            for j, resp2 in enumerate(responses[i+1:], i+1):
                if resp1['model'] != resp2['model'] or \
                   resp1['params'] != resp2['params']:
                    different_source_pairs.append((i, j))
        
        # éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†
        selected_pairs = random.sample(
            different_source_pairs,
            min(len(different_source_pairs), self.config.max_pairs_per_prompt)
        )
        
        for i, j in selected_pairs:
            pairs.append((responses[i], responses[j]))
            response_coverage[i] += 1
            response_coverage[j] += 1
        
        # 3. è¡¥å……æœªè¦†ç›–çš„å“åº”
        uncovered = [i for i, count in response_coverage.items() if count == 0]
        for idx in uncovered:
            # éšæœºé€‰æ‹©ä¸€ä¸ªé…å¯¹
            other_idx = random.choice([i for i in range(len(responses)) if i != idx])
            pairs.append((responses[idx], responses[other_idx]))
        
        return pairs
    
    def distribute_to_annotators(self, tasks):
        """åˆ†é…æ ‡æ³¨ä»»åŠ¡"""
        annotations = []
        
        # æŒ‰æ ‡æ³¨è€…èƒ½åŠ›å’Œè´Ÿè½½åˆ†é…
        for task in tasks:
            # é€‰æ‹©åˆé€‚çš„æ ‡æ³¨è€…
            suitable_annotators = self.annotator_pool.get_suitable_annotators(
                task_type='comparison',
                domain=self.classify_domain(task.prompt),
                num_needed=self.config.annotators_per_task
            )
            
            task_annotations = []
            for annotator in suitable_annotators:
                # å‘é€ä»»åŠ¡
                annotation = annotator.annotate(task)
                task_annotations.append({
                    'annotator_id': annotator.id,
                    'annotation': annotation,
                    'metadata': {
                        'time_taken': annotation.time_taken,
                        'confidence': annotation.confidence
                    }
                })
            
            annotations.append({
                'task': task,
                'annotations': task_annotations
            })
        
        return annotations
    
    def validate_annotations(self, annotations):
        """éªŒè¯æ ‡æ³¨è´¨é‡"""
        valid_annotations = []
        
        for annotation_set in annotations:
            task = annotation_set['task']
            task_annotations = annotation_set['annotations']
            
            # 1. æ£€æŸ¥æ ‡æ³¨è€…é—´ä¸€è‡´æ€§
            preferences = [a['annotation'].preference for a in task_annotations]
            agreement_score = self.compute_agreement(preferences)
            
            if agreement_score < self.config.min_agreement:
                # éœ€è¦é¢å¤–æ ‡æ³¨
                self.request_additional_annotations(task)
                continue
            
            # 2. æ£€æŸ¥æ ‡æ³¨è´¨é‡
            quality_scores = []
            for ann in task_annotations:
                quality = self.assess_annotation_quality(ann)
                quality_scores.append(quality)
            
            avg_quality = np.mean(quality_scores)
            if avg_quality < self.config.min_quality:
                continue
            
            # 3. èšåˆæ ‡æ³¨ç»“æœ
            final_preference = self.aggregate_preferences(preferences)
            
            valid_annotations.append({
                'prompt': task.prompt,
                'response_a': task.response_a,
                'response_b': task.response_b,
                'preference': final_preference,
                'agreement': agreement_score,
                'quality': avg_quality,
                'metadata': {
                    'annotator_count': len(task_annotations),
                    'individual_annotations': task_annotations
                }
            })
        
        return valid_annotations
    
    def compute_agreement(self, preferences):
        """è®¡ç®—æ ‡æ³¨è€…ä¸€è‡´æ€§"""
        if len(preferences) < 2:
            return 1.0
        
        # ä½¿ç”¨Krippendorff's alpha
        # ç®€åŒ–ç‰ˆï¼šè®¡ç®—å¤šæ•°æŠ•ç¥¨çš„æ¯”ä¾‹
        counts = Counter(preferences)
        majority_count = max(counts.values())
        agreement = majority_count / len(preferences)
        
        return agreement
    
    def assess_annotation_quality(self, annotation):
        """è¯„ä¼°å•ä¸ªæ ‡æ³¨çš„è´¨é‡"""
        quality_factors = []
        
        # 1. æ—¶é—´åˆç†æ€§
        time_taken = annotation['metadata']['time_taken']
        if self.config.min_time <= time_taken <= self.config.max_time:
            quality_factors.append(1.0)
        else:
            quality_factors.append(0.5)
        
        # 2. ç½®ä¿¡åº¦
        confidence = annotation['metadata']['confidence']
        quality_factors.append(confidence)
        
        # 3. æ ‡æ³¨è€…å†å²å‡†ç¡®ç‡
        annotator_id = annotation['annotator_id']
        historical_accuracy = self.annotator_pool.get_accuracy(annotator_id)
        quality_factors.append(historical_accuracy)
        
        # 4. ç†ç”±è´¨é‡ï¼ˆå¦‚æœæä¾›ï¼‰
        if 'reasoning' in annotation['annotation']:
            reasoning_quality = self.assess_reasoning(
                annotation['annotation'].reasoning
            )
            quality_factors.append(reasoning_quality)
        
        return np.mean(quality_factors)
    
    def aggregate_preferences(self, preferences):
        """èšåˆå¤šä¸ªæ ‡æ³¨è€…çš„åå¥½"""
        # åŠ æƒæŠ•ç¥¨
        weighted_sum = 0
        total_weight = 0
        
        for i, pref in enumerate(preferences):
            # å¯ä»¥æ ¹æ®æ ‡æ³¨è€…è´¨é‡åŠ æƒ
            weight = 1.0  # ç®€åŒ–ç‰ˆï¼šç­‰æƒé‡
            weighted_sum += pref * weight
            total_weight += weight
        
        return weighted_sum / total_weight
    
    def store_preferences(self, valid_annotations):
        """å­˜å‚¨éªŒè¯åçš„åå¥½æ•°æ®"""
        for annotation in valid_annotations:
            self.preference_database.add(
                prompt=annotation['prompt'],
                chosen=annotation['response_a']['text'] if annotation['preference'] > 0.5 
                       else annotation['response_b']['text'],
                rejected=annotation['response_b']['text'] if annotation['preference'] > 0.5 
                         else annotation['response_a']['text'],
                metadata={
                    'preference_strength': abs(annotation['preference'] - 0.5) * 2,
                    'agreement': annotation['agreement'],
                    'quality': annotation['quality'],
                    'collected_at': datetime.now()
                }
            )
    
    def update_statistics(self, annotations):
        """æ›´æ–°æ”¶é›†ç»Ÿè®¡"""
        # æ›´æ–°promptç»Ÿè®¡
        for ann in annotations:
            self.prompt_bank.update_prompt_stats(
                ann['prompt'],
                collected_comparisons=1,
                avg_agreement=ann['agreement']
            )
        
        # æ›´æ–°æ ‡æ³¨è€…ç»Ÿè®¡
        for ann in annotations:
            for individual in ann['metadata']['individual_annotations']:
                self.annotator_pool.update_annotator_stats(
                    individual['annotator_id'],
                    tasks_completed=1,
                    avg_time=individual['metadata']['time_taken']
                )
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
# é…ç½®
config = DataCollectionConfig(
    batch_size=100,
    responses_per_prompt=4,
    models_per_prompt=2,
    min_responses=3,
    max_pairs_per_prompt=6,
    annotators_per_task=3,
    min_agreement=0.7,
    min_quality=0.8,
    min_time=10,  # ç§’
    max_time=300,  # ç§’
    quality_thresholds={
        'reasoning_min_length': 20,
        'confidence_threshold': 0.6
    }
)

# åˆå§‹åŒ–pipeline
pipeline = RLHFDataCollectionPipeline(
    models=[model1, model2, model3],
    annotator_pool=annotator_pool,
    config=config
)

# æ”¶é›†åå¥½æ•°æ®
preference_data = pipeline.collect_preferences(num_comparisons=10000)

# æ•°æ®ç»Ÿè®¡
print(f"Collected {len(preference_data)} valid comparisons")
print(f"Average agreement: {np.mean([p['metadata']['agreement'] for p in preference_data])}")
print(f"Average quality: {np.mean([p['metadata']['quality'] for p in preference_data])}")
```

</details>

### 4.2.6 RLHFçš„å®éªŒè¿½è¸ª

**ç»¼åˆå®éªŒç®¡ç†ï¼š**
```python
class RLHFExperimentTracker:
    def __init__(self, experiment_name):
        self.experiment_name = experiment_name
        self.metrics_history = defaultdict(list)
        self.checkpoints = []
        
    def log_sft_metrics(self, epoch, metrics):
        for key, value in metrics.items():
            self.metrics_history[f'sft/{key}'].append({
                'epoch': epoch,
                'value': value
            })
    
    def log_rm_metrics(self, step, metrics):
        for key, value in metrics.items():
            self.metrics_history[f'rm/{key}'].append({
                'step': step,
                'value': value
            })
    
    def log_ppo_metrics(self, step, metrics):
        for key, value in metrics.items():
            self.metrics_history[f'ppo/{key}'].append({
                'step': step,
                'value': value
            })
    
    def save_checkpoint(self, models, step, metrics):
        checkpoint = {
            'step': step,
            'models': {
                'policy': models['policy'].state_dict(),
                'value': models.get('value', {}).state_dict() if 'value' in models else None,
                'reward': models.get('reward', {}).state_dict() if 'reward' in models else None
            },
            'metrics': metrics,
            'timestamp': datetime.now()
        }
        
        self.checkpoints.append(checkpoint)
        
        # ä¿å­˜åˆ°ç£ç›˜
        torch.save(
            checkpoint,
            f"{self.experiment_name}_checkpoint_{step}.pt"
        )
```

**ğŸ”¬ ç ”ç©¶çº¿ç´¢ï¼š**
- å¦‚ä½•å‡å°‘RLHFæ‰€éœ€çš„äººç±»æ ‡æ³¨é‡ï¼Ÿ
- èƒ½å¦ç”¨AIåé¦ˆæ›¿ä»£éƒ¨åˆ†äººç±»åé¦ˆï¼Ÿ
- å¦‚ä½•å¤„ç†æ ‡æ³¨è€…ä¹‹é—´çš„ä»·å€¼è§‚å·®å¼‚ï¼Ÿ

---

## <a name="section3"></a>4.3 å¥–åŠ±æ¨¡å‹çš„è®¾è®¡ä¸è®­ç»ƒ

å¥–åŠ±æ¨¡å‹æ˜¯RLHFçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒå­¦ä¹ äººç±»åå¥½å¹¶æŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ã€‚æœ¬èŠ‚æ·±å…¥æ¢è®¨å¥–åŠ±æ¨¡å‹çš„è®¾è®¡ç»†èŠ‚ã€‚

### 4.3.1 å¥–åŠ±æ¨¡å‹çš„ç†è®ºåŸºç¡€

**Bradley-Terryæ¨¡å‹ï¼š**
äººç±»åå¥½å¯ä»¥å»ºæ¨¡ä¸ºï¼š
$$P(y_1 \succ y_2 | x) = \frac{\exp(r(x, y_1))}{\exp(r(x, y_1)) + \exp(r(x, y_2))} = \sigma(r(x, y_1) - r(x, y_2))$$

å…¶ä¸­ $r(x, y)$ æ˜¯å¥–åŠ±å‡½æ•°ï¼Œ$\sigma$ æ˜¯sigmoidå‡½æ•°ã€‚

**Plackett-Luceæ¨¡å‹ï¼ˆå¤šé€‰é¡¹æ’åºï¼‰ï¼š**
$$P(\tau | x) = \prod_{i=1}^{K-1} \frac{\exp(r(x, y_{\tau(i)}))}{\sum_{j=i}^{K} \exp(r(x, y_{\tau(j)}))}$$

### 4.3.2 å¥–åŠ±æ¨¡å‹æ¶æ„è®¾è®¡

**åŸºç¡€æ¶æ„ï¼š**
```python
class RewardModel(nn.Module):
    def __init__(self, base_model_name, config):
        super().__init__()
        self.config = config
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºbackbone
        self.backbone = AutoModel.from_pretrained(base_model_name)
        
        # å†»ç»“éƒ¨åˆ†å±‚ï¼ˆå¯é€‰ï¼‰
        if config.freeze_layers > 0:
            self.freeze_backbone_layers(config.freeze_layers)
        
        # å¥–åŠ±é¢„æµ‹å¤´
        self.reward_head = self.build_reward_head(
            self.backbone.config.hidden_size
        )
        
        # è¾…åŠ©ä»»åŠ¡å¤´ï¼ˆå¯é€‰ï¼‰
        if config.use_auxiliary_tasks:
            self.auxiliary_heads = self.build_auxiliary_heads()
    
    def build_reward_head(self, hidden_size):
        if self.config.reward_head_type == 'linear':
            return nn.Linear(hidden_size, 1)
        elif self.config.reward_head_type == 'mlp':
            return nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_size // 2, 1)
            )
        elif self.config.reward_head_type == 'attention':
            return AttentionPoolingHead(hidden_size)
    
    def forward(self, input_ids, attention_mask=None, return_all=False):
        # è·å–backboneè¾“å‡º
        outputs = self.backbone(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        # é€‰æ‹©èšåˆç­–ç•¥
        if self.config.pooling_strategy == 'last':
            # ä½¿ç”¨æœ€åä¸€ä¸ªtoken
            hidden = outputs.hidden_states[-1][:, -1, :]
        elif self.config.pooling_strategy == 'mean':
            # å¹³å‡æ± åŒ–
            hidden = self.mean_pooling(
                outputs.hidden_states[-1],
                attention_mask
            )
        elif self.config.pooling_strategy == 'max':
            # æœ€å¤§æ± åŒ–
            hidden = self.max_pooling(
                outputs.hidden_states[-1],
                attention_mask
            )
        elif self.config.pooling_strategy == 'cls':
            # ä½¿ç”¨CLS tokenï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
            hidden = outputs.hidden_states[-1][:, 0, :]
        
        # è®¡ç®—å¥–åŠ±
        reward = self.reward_head(hidden)
        
        if return_all:
            results = {'reward': reward, 'hidden': hidden}
            if hasattr(self, 'auxiliary_heads'):
                for name, head in self.auxiliary_heads.items():
                    results[name] = head(hidden)
            return results
        
        return reward
    
    def mean_pooling(self, hidden_states, attention_mask):
        # è€ƒè™‘attention maskçš„å¹³å‡æ± åŒ–
        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size())
        sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)
        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)
        return sum_embeddings / sum_mask
```

**é«˜çº§ç‰¹å¾æå–ï¼š**
```python
class AttentionPoolingHead(nn.Module):
    """ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶èšåˆåºåˆ—ç‰¹å¾"""
    def __init__(self, hidden_size):
        super().__init__()
        self.attention = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 4),
            nn.Tanh(),
            nn.Linear(hidden_size // 4, 1)
        )
        self.reward_projection = nn.Linear(hidden_size, 1)
    
    def forward(self, hidden_states, mask=None):
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_scores = self.attention(hidden_states).squeeze(-1)
        
        if mask is not None:
            attn_scores = attn_scores.masked_fill(~mask, -float('inf'))
        
        attn_weights = F.softmax(attn_scores, dim=1)
        
        # åŠ æƒèšåˆ
        weighted_hidden = torch.sum(
            hidden_states * attn_weights.unsqueeze(-1),
            dim=1
        )
        
        # è®¡ç®—å¥–åŠ±
        reward = self.reward_projection(weighted_hidden)
        
        return reward
```

### 4.3.3 è®­ç»ƒæ•°æ®å‡†å¤‡

**æ•°æ®æ ¼å¼ä¸é¢„å¤„ç†ï¼š**
```python
class RewardModelDataset(Dataset):
    def __init__(self, preference_data, tokenizer, max_length=512):
        self.data = preference_data
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # ç¼–ç chosenå“åº”
        chosen_text = item['prompt'] + item['chosen']
        chosen_encoding = self.tokenizer(
            chosen_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # ç¼–ç rejectedå“åº”
        rejected_text = item['prompt'] + item['rejected']
        rejected_encoding = self.tokenizer(
            rejected_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'chosen_input_ids': chosen_encoding['input_ids'].squeeze(),
            'chosen_attention_mask': chosen_encoding['attention_mask'].squeeze(),
            'rejected_input_ids': rejected_encoding['input_ids'].squeeze(),
            'rejected_attention_mask': rejected_encoding['attention_mask'].squeeze(),
            'margin': item.get('margin', 1.0)  # åå¥½å¼ºåº¦
        }
```

**æ•°æ®å¢å¼ºæŠ€æœ¯ï¼š**
```python
class RewardDataAugmenter:
    def __init__(self, augmentation_config):
        self.config = augmentation_config
        
    def augment_dataset(self, original_data):
        augmented_data = original_data.copy()
        
        if self.config.use_paraphrasing:
            augmented_data.extend(self.paraphrase_augmentation(original_data))
        
        if self.config.use_backtranslation:
            augmented_data.extend(self.backtranslation_augmentation(original_data))
        
        if self.config.use_preference_interpolation:
            augmented_data.extend(self.interpolate_preferences(original_data))
        
        if self.config.use_transitivity:
            augmented_data.extend(self.transitivity_augmentation(original_data))
        
        return augmented_data
    
    def transitivity_augmentation(self, data):
        """åˆ©ç”¨åå¥½çš„ä¼ é€’æ€§ç”Ÿæˆæ–°æ•°æ®"""
        # å»ºç«‹åå¥½å›¾
        preference_graph = defaultdict(list)
        
        for item in data:
            key = (item['prompt'], item['chosen'])
            preference_graph[key].append(item['rejected'])
        
        # æ‰¾å‡ºä¼ é€’å…³ç³»
        new_comparisons = []
        for (prompt, a), b_list in preference_graph.items():
            for b in b_list:
                # æŸ¥æ‰¾ b > c çš„å…³ç³»
                key_b = (prompt, b)
                if key_b in preference_graph:
                    for c in preference_graph[key_b]:
                        # é€šè¿‡ä¼ é€’æ€§ï¼ša > b ä¸” b > c => a > c
                        new_comparisons.append({
                            'prompt': prompt,
                            'chosen': a,
                            'rejected': c,
                            'margin': 0.8,  # ä¼ é€’æ€§åå¥½é€šå¸¸è¾ƒå¼±
                            'source': 'transitivity'
                        })
        
        return new_comparisons
```

### 4.3.4 æŸå¤±å‡½æ•°è®¾è®¡

**åŸºç¡€æ’åºæŸå¤±ï¼š**
```python
def ranking_loss(chosen_rewards, rejected_rewards, margin=0.0):
    """Pairwise ranking loss with optional margin"""
    return F.relu(margin - (chosen_rewards - rejected_rewards)).mean()
```

**åŠ æƒBradley-TerryæŸå¤±ï¼š**
```python
def weighted_bt_loss(chosen_rewards, rejected_rewards, weights=None):
    """Bradley-Terry loss with importance weights"""
    logits = chosen_rewards - rejected_rewards
    
    if weights is not None:
        loss = -(F.logsigmoid(logits) * weights).mean()
    else:
        loss = -F.logsigmoid(logits).mean()
    
    return loss
```

**å¤šä»»åŠ¡å­¦ä¹ æŸå¤±ï¼š**
```python
class MultiTaskRewardLoss(nn.Module):
    def __init__(self, task_weights):
        super().__init__()
        self.task_weights = task_weights
        
    def forward(self, model_outputs, targets):
        total_loss = 0
        losses = {}
        
        # ä¸»è¦ä»»åŠ¡ï¼šåå¥½é¢„æµ‹
        preference_loss = weighted_bt_loss(
            model_outputs['chosen_rewards'],
            model_outputs['rejected_rewards'],
            targets.get('preference_weights')
        )
        total_loss += self.task_weights['preference'] * preference_loss
        losses['preference'] = preference_loss
        
        # è¾…åŠ©ä»»åŠ¡1ï¼šæœ‰ç”¨æ€§é¢„æµ‹
        if 'helpfulness' in model_outputs:
            helpfulness_loss = F.mse_loss(
                model_outputs['helpfulness'].squeeze(),
                targets['helpfulness_scores']
            )
            total_loss += self.task_weights['helpfulness'] * helpfulness_loss
            losses['helpfulness'] = helpfulness_loss
        
        # è¾…åŠ©ä»»åŠ¡2ï¼šå®‰å…¨æ€§é¢„æµ‹
        if 'safety' in model_outputs:
            safety_loss = F.binary_cross_entropy_with_logits(
                model_outputs['safety'].squeeze(),
                targets['safety_labels']
            )
            total_loss += self.task_weights['safety'] * safety_loss
            losses['safety'] = safety_loss
        
        return total_loss, losses
```

### 4.3.5 è®­ç»ƒç­–ç•¥ä¸æŠ€å·§

**è¯¾ç¨‹å­¦ä¹ ï¼š**
```python
class CurriculumRewardTrainer:
    def __init__(self, model, dataset, config):
        self.model = model
        self.dataset = dataset
        self.config = config
        
        # éš¾åº¦è¯„ä¼°å™¨
        self.difficulty_scorer = DifficultyScorer()
        
    def train(self):
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„éš¾åº¦
        difficulties = []
        for item in self.dataset:
            score = self.difficulty_scorer.score(item)
            difficulties.append(score)
        
        # æŒ‰éš¾åº¦æ’åº
        sorted_indices = np.argsort(difficulties)
        
        # åˆ†é˜¶æ®µè®­ç»ƒ
        stages = self.config.curriculum_stages
        for stage in range(stages):
            # é€‰æ‹©å½“å‰é˜¶æ®µçš„æ•°æ®
            start_idx = int(len(sorted_indices) * stage / stages)
            end_idx = int(len(sorted_indices) * (stage + 1) / stages)
            
            stage_indices = sorted_indices[:end_idx]  # ç´¯ç§¯å¼
            stage_dataset = Subset(self.dataset, stage_indices)
            
            # è®­ç»ƒå½“å‰é˜¶æ®µ
            self.train_stage(stage_dataset, stage)
    
    def train_stage(self, dataset, stage):
        # åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
        lr = self.config.base_lr * (self.config.lr_decay ** stage)
        optimizer = AdamW(self.model.parameters(), lr=lr)
        
        dataloader = DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=True
        )
        
        for epoch in range(self.config.epochs_per_stage):
            for batch in dataloader:
                loss = self.compute_loss(batch)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
```

**å¯¹æ¯”å­¦ä¹ å¢å¼ºï¼š**
```python
class ContrastiveRewardModel(RewardModel):
    def __init__(self, base_model_name, config):
        super().__init__(base_model_name, config)
        
        # å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´
        self.projection_head = nn.Sequential(
            nn.Linear(self.backbone.config.hidden_size, 256),
            nn.ReLU(),
            nn.Linear(256, 128)
        )
        
        self.temperature = config.temperature
    
    def compute_contrastive_loss(self, embeddings, labels):
        """è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±"""
        # å½’ä¸€åŒ–åµŒå…¥
        embeddings = F.normalize(embeddings, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature
        
        # åˆ›å»ºæ ‡ç­¾çŸ©é˜µï¼ˆç›¸åŒpromptçš„ä¸ºæ­£æ ·æœ¬ï¼‰
        labels = labels.view(-1, 1)
        label_matrix = labels == labels.T
        
        # è®¡ç®—æŸå¤±
        exp_sim = torch.exp(similarity_matrix)
        
        # åˆ†å­ï¼šæ­£æ ·æœ¬å¯¹
        positive_sim = exp_sim * label_matrix
        
        # åˆ†æ¯ï¼šæ‰€æœ‰æ ·æœ¬å¯¹ï¼ˆé™¤äº†è‡ªå·±ï¼‰
        mask = torch.eye(len(labels), dtype=torch.bool, device=labels.device)
        negative_sim = exp_sim.masked_fill(mask, 0).sum(dim=1, keepdim=True)
        
        loss = -torch.log(positive_sim.sum(dim=1) / (positive_sim.sum(dim=1) + negative_sim))
        
        return loss.mean()
```

#### ç»ƒä¹  4.3ï¼šå®ç°é›†æˆå¥–åŠ±æ¨¡å‹
è®¾è®¡å¹¶å®ç°ä¸€ä¸ªé›†æˆå¤šä¸ªå¥–åŠ±æ¨¡å‹çš„ç³»ç»Ÿï¼Œæé«˜é¢„æµ‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

<details>
<summary>æŸ¥çœ‹ç­”æ¡ˆ</summary>

**é›†æˆå¥–åŠ±æ¨¡å‹å®ç°ï¼š**

```python
class EnsembleRewardModel:
    def __init__(self, model_configs, ensemble_config):
        self.models = []
        self.ensemble_config = ensemble_config
        
        # åˆå§‹åŒ–å„ä¸ªåŸºæ¨¡å‹
        for config in model_configs:
            model = self.create_model(config)
            self.models.append({
                'model': model,
                'weight': config.get('weight', 1.0),
                'type': config['type']
            })
        
        # é›†æˆç­–ç•¥
        self.ensemble_method = ensemble_config.ensemble_method
        
        # ä¸ç¡®å®šæ€§ä¼°è®¡
        self.uncertainty_estimator = UncertaintyEstimator()
        
        # æ¨¡å‹é€‰æ‹©å™¨ï¼ˆç”¨äºåŠ¨æ€é›†æˆï¼‰
        if self.ensemble_method == 'dynamic':
            self.model_selector = ModelSelector(len(self.models))
    
    def create_model(self, config):
        """åˆ›å»ºå•ä¸ªå¥–åŠ±æ¨¡å‹"""
        if config['type'] == 'standard':
            model = RewardModel(config['base_model'], config)
        elif config['type'] == 'multitask':
            model = MultiTaskRewardModel(config['base_model'], config)
        elif config['type'] == 'contrastive':
            model = ContrastiveRewardModel(config['base_model'], config)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if 'checkpoint' in config:
            model.load_state_dict(torch.load(config['checkpoint']))
        
        return model
    
    def predict(self, input_ids, attention_mask=None, return_uncertainty=False):
        """é›†æˆé¢„æµ‹"""
        all_predictions = []
        all_features = []
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        with torch.no_grad():
            for model_info in self.models:
                model = model_info['model']
                model.eval()
                
                outputs = model(
                    input_ids,
                    attention_mask,
                    return_all=True
                )
                
                all_predictions.append(outputs['reward'])
                all_features.append(outputs['hidden'])
        
        # åº”ç”¨é›†æˆç­–ç•¥
        if self.ensemble_method == 'mean':
            ensemble_reward = self.weighted_mean_ensemble(all_predictions)
        elif self.ensemble_method == 'median':
            ensemble_reward = self.median_ensemble(all_predictions)
        elif self.ensemble_method == 'trimmed_mean':
            ensemble_reward = self.trimmed_mean_ensemble(all_predictions)
        elif self.ensemble_method == 'dynamic':
            ensemble_reward = self.dynamic_ensemble(
                all_predictions,
                all_features,
                input_ids
            )
        elif self.ensemble_method == 'stacking':
            ensemble_reward = self.stacking_ensemble(all_predictions)
        
        results = {'reward': ensemble_reward}
        
        # è®¡ç®—ä¸ç¡®å®šæ€§
        if return_uncertainty:
            uncertainty = self.uncertainty_estimator.estimate(
                all_predictions,
                method=self.ensemble_config.uncertainty_method
            )
            results['uncertainty'] = uncertainty
        
        return results
    
    def weighted_mean_ensemble(self, predictions):
        """åŠ æƒå¹³å‡é›†æˆ"""
        weighted_sum = 0
        total_weight = 0
        
        for i, pred in enumerate(predictions):
            weight = self.models[i]['weight']
            weighted_sum += pred * weight
            total_weight += weight
        
        return weighted_sum / total_weight
    
    def median_ensemble(self, predictions):
        """ä¸­ä½æ•°é›†æˆï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼‰"""
        stacked = torch.stack(predictions, dim=0)
        return torch.median(stacked, dim=0)[0]
    
    def trimmed_mean_ensemble(self, predictions, trim_ratio=0.2):
        """å»é™¤æå€¼çš„å¹³å‡"""
        stacked = torch.stack(predictions, dim=0)
        n_models = len(predictions)
        n_trim = int(n_models * trim_ratio)
        
        if n_trim > 0:
            sorted_preds, _ = torch.sort(stacked, dim=0)
            trimmed = sorted_preds[n_trim:-n_trim]
            return trimmed.mean(dim=0)
        else:
            return stacked.mean(dim=0)
    
    def dynamic_ensemble(self, predictions, features, input_ids):
        """åŠ¨æ€é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹"""
        # åŸºäºè¾“å…¥ç‰¹å¾é€‰æ‹©æƒé‡
        input_features = self.extract_input_features(input_ids)
        
        # é¢„æµ‹æ¯ä¸ªæ¨¡å‹çš„å¯é æ€§
        model_weights = self.model_selector(input_features)
        
        # åŠ æƒç»„åˆ
        weighted_sum = 0
        for i, pred in enumerate(predictions):
            weighted_sum += pred * model_weights[i]
        
        return weighted_sum
    
    def stacking_ensemble(self, predictions):
        """ä½¿ç”¨å…ƒå­¦ä¹ å™¨çš„å †å é›†æˆ"""
        # å°†æ‰€æœ‰é¢„æµ‹ä½œä¸ºç‰¹å¾
        stacked_features = torch.stack(predictions, dim=-1)
        
        # ä½¿ç”¨å…ƒæ¨¡å‹
        if not hasattr(self, 'meta_model'):
            raise ValueError("Meta model not initialized for stacking")
        
        ensemble_reward = self.meta_model(stacked_features)
        
        return ensemble_reward
    
    def train_ensemble(self, dataset, validation_data):
        """è®­ç»ƒé›†æˆæ¨¡å‹"""
        # 1. ç‹¬ç«‹è®­ç»ƒæ¯ä¸ªåŸºæ¨¡å‹
        for i, model_info in enumerate(self.models):
            print(f"Training model {i+1}/{len(self.models)}")
            
            # å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ•°æ®å­é›†æˆ–å¢å¼º
            if self.ensemble_config.use_bagging:
                train_subset = self.create_bootstrap_sample(dataset)
            else:
                train_subset = dataset
            
            self.train_single_model(
                model_info['model'],
                train_subset,
                validation_data
            )
        
        # 2. å¦‚æœä½¿ç”¨stackingï¼Œè®­ç»ƒå…ƒæ¨¡å‹
        if self.ensemble_method == 'stacking':
            self.train_meta_model(validation_data)
        
        # 3. å¦‚æœä½¿ç”¨åŠ¨æ€é›†æˆï¼Œè®­ç»ƒé€‰æ‹©å™¨
        if self.ensemble_method == 'dynamic':
            self.train_model_selector(validation_data)
    
    def train_single_model(self, model, train_data, val_data):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        optimizer = AdamW(model.parameters(), lr=1e-5)
        best_val_acc = 0
        patience_counter = 0
        
        for epoch in range(self.ensemble_config.epochs_per_model):
            # è®­ç»ƒ
            model.train()
            for batch in DataLoader(train_data, batch_size=32, shuffle=True):
                optimizer.zero_grad()
                
                chosen_rewards = model(
                    batch['chosen_input_ids'],
                    batch['chosen_attention_mask']
                )
                rejected_rewards = model(
                    batch['rejected_input_ids'],
                    batch['rejected_attention_mask']
                )
                
                loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()
                loss.backward()
                optimizer.step()
            
            # éªŒè¯
            val_acc = self.validate_model(model, val_data)
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(model.state_dict(), f'best_model_{id(model)}.pt')
            else:
                patience_counter += 1
                
            if patience_counter > self.ensemble_config.patience:
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        model.load_state_dict(torch.load(f'best_model_{id(model)}.pt'))
    
    def create_bootstrap_sample(self, dataset, sample_ratio=0.8):
        """åˆ›å»ºè‡ªåŠ©é‡‡æ ·å­é›†"""
        n_samples = int(len(dataset) * sample_ratio)
        indices = np.random.choice(len(dataset), n_samples, replace=True)
        return Subset(dataset, indices)
```

**ä¸ç¡®å®šæ€§ä¼°è®¡å™¨ï¼š**
```python
class UncertaintyEstimator:
    def estimate(self, predictions, method='std'):
        """ä¼°è®¡é¢„æµ‹çš„ä¸ç¡®å®šæ€§"""
        stacked = torch.stack(predictions, dim=0)
        
        if method == 'std':
            # æ ‡å‡†å·®
            uncertainty = torch.std(stacked, dim=0)
        elif method == 'entropy':
            # é¢„æµ‹åˆ†å¸ƒçš„ç†µ
            probs = F.softmax(stacked, dim=0)
            uncertainty = -torch.sum(probs * torch.log(probs + 1e-8), dim=0)
        elif method == 'variance_ratio':
            # æ–¹å·®æ¯”
            mean_pred = stacked.mean(dim=0)
            variance = torch.var(stacked, dim=0)
            uncertainty = variance / (torch.abs(mean_pred) + 1e-8)
        elif method == 'mc_dropout':
            # MC Dropoutï¼ˆéœ€è¦åœ¨é¢„æµ‹æ—¶å¯ç”¨dropoutï¼‰
            uncertainty = self.mc_dropout_uncertainty(predictions)
        
        return uncertainty
    
    def mc_dropout_uncertainty(self, predictions, n_samples=10):
        """ä½¿ç”¨MC Dropoutä¼°è®¡ä¸ç¡®å®šæ€§"""
        # è¿™é‡Œpredictionsåº”è¯¥åŒ…å«å¤šæ¬¡å‰å‘ä¼ æ’­çš„ç»“æœ
        if len(predictions) < n_samples:
            raise ValueError(f"Need at least {n_samples} predictions for MC Dropout")
        
        stacked = torch.stack(predictions[:n_samples], dim=0)
        return torch.std(stacked, dim=0)
```

**æ¨¡å‹é€‰æ‹©å™¨ï¼ˆç”¨äºåŠ¨æ€é›†æˆï¼‰ï¼š**
```python
class ModelSelector(nn.Module):
    def __init__(self, n_models, input_dim=768):
        super().__init__()
        self.n_models = n_models
        
        # è¾“å…¥ç‰¹å¾æå–
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU()
        )
        
        # æ¨¡å‹æƒé‡é¢„æµ‹
        self.weight_predictor = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, n_models),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, input_features):
        features = self.feature_extractor(input_features)
        weights = self.weight_predictor(features)
        return weights
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
# é…ç½®å¤šä¸ªæ¨¡å‹
model_configs = [
    {
        'type': 'standard',
        'base_model': 'gpt2-medium',
        'pooling_strategy': 'last',
        'weight': 1.0
    },
    {
        'type': 'multitask',
        'base_model': 'gpt2-medium',
        'pooling_strategy': 'mean',
        'weight': 0.8
    },
    {
        'type': 'contrastive',
        'base_model': 'gpt2-large',
        'pooling_strategy': 'attention',
        'weight': 1.2
    }
]

# é›†æˆé…ç½®
ensemble_config = {
    'ensemble_method': 'dynamic',  # 'mean', 'median', 'trimmed_mean', 'dynamic', 'stacking'
    'uncertainty_method': 'variance_ratio',
    'use_bagging': True,
    'epochs_per_model': 3,
    'patience': 3
}

# åˆ›å»ºé›†æˆæ¨¡å‹
ensemble_model = EnsembleRewardModel(model_configs, ensemble_config)

# è®­ç»ƒ
ensemble_model.train_ensemble(train_dataset, val_dataset)

# é¢„æµ‹
results = ensemble_model.predict(
    input_ids,
    attention_mask,
    return_uncertainty=True
)

print(f"Reward: {results['reward']}")
print(f"Uncertainty: {results['uncertainty']}")
```

</details>

### 4.3.6 å¥–åŠ±æ¨¡å‹çš„è¯„ä¼°

**è¯„ä¼°æŒ‡æ ‡ï¼š**
```python
class RewardModelEvaluator:
    def __init__(self, model, test_dataset):
        self.model = model
        self.test_dataset = test_dataset
        
    def evaluate(self):
        metrics = {}
        
        # 1. å‡†ç¡®ç‡
        metrics['accuracy'] = self.compute_accuracy()
        
        # 2. æ’åºç›¸å…³æ€§
        metrics['ranking_correlation'] = self.compute_ranking_correlation()
        
        # 3. æ ¡å‡†åº¦
        metrics['calibration'] = self.compute_calibration()
        
        # 4. é²æ£’æ€§
        metrics['robustness'] = self.compute_robustness()
        
        return metrics
    
    def compute_accuracy(self):
        """è®¡ç®—åå¥½é¢„æµ‹å‡†ç¡®ç‡"""
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch in DataLoader(self.test_dataset, batch_size=32):
                chosen_rewards = self.model(
                    batch['chosen_input_ids'],
                    batch['chosen_attention_mask']
                )
                rejected_rewards = self.model(
                    batch['rejected_input_ids'],
                    batch['rejected_attention_mask']
                )
                
                predictions = chosen_rewards > rejected_rewards
                correct += predictions.sum().item()
                total += len(predictions)
        
        return correct / total
    
    def compute_ranking_correlation(self):
        """è®¡ç®—ä¸äººç±»æ’åºçš„ç›¸å…³æ€§"""
        human_rankings = []
        model_rankings = []
        
        # æ”¶é›†æ’åºæ•°æ®
        for prompt_group in self.test_dataset.get_ranking_groups():
            # äººç±»æ’åº
            human_order = prompt_group['human_ranking']
            human_rankings.append(human_order)
            
            # æ¨¡å‹é¢„æµ‹åˆ†æ•°
            scores = []
            for response in prompt_group['responses']:
                score = self.model(response['input_ids']).item()
                scores.append(score)
            
            # æ¨¡å‹æ’åº
            model_order = np.argsort(scores)[::-1]
            model_rankings.append(model_order)
        
        # è®¡ç®—Kendall's tau
        from scipy.stats import kendalltau
        correlations = []
        for human, model in zip(human_rankings, model_rankings):
            tau, _ = kendalltau(human, model)
            correlations.append(tau)
        
        return np.mean(correlations)
```

**ğŸ”¬ ç ”ç©¶çº¿ç´¢ï¼š**
- å¦‚ä½•è®¾è®¡å¯¹åˆ†å¸ƒå¤–æ•°æ®æ›´é²æ£’çš„å¥–åŠ±æ¨¡å‹ï¼Ÿ
- èƒ½å¦ç”¨ä¸»åŠ¨å­¦ä¹ å‡å°‘æ‰€éœ€çš„åå¥½æ ‡æ³¨ï¼Ÿ
- å¦‚ä½•å¤„ç†å¥–åŠ±æ¨¡å‹çš„è¿‡åº¦ä¼˜åŒ–é—®é¢˜ï¼Ÿ

---

## <a name="section4"></a>4.4 PPO vs DPO vs IPOç®—æ³•å¯¹æ¯”

é™¤äº†PPOï¼Œè¿˜æœ‰å…¶ä»–æ–¹æ³•å¯ä»¥ä»äººç±»åé¦ˆä¸­å­¦ä¹ ã€‚æœ¬èŠ‚æ¯”è¾ƒä¸»æµçš„å¯¹é½ç®—æ³•ã€‚

### 4.4.1 ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰

**DPOçš„æ ¸å¿ƒæ€æƒ³ï¼š**
å°†RLHFçš„å¥–åŠ±å»ºæ¨¡å’ŒRLä¼˜åŒ–åˆå¹¶ä¸ºå•ä¸€ç›®æ ‡ã€‚

**ç†è®ºæ¨å¯¼ï¼š**
ä»RLç›®æ ‡å‡ºå‘ï¼š
$$\max_{\pi} \mathbb{E}_{x \sim D, y \sim \pi}[r(x,y)] - \beta KL[\pi || \pi_{ref}]$$

å¯ä»¥æ¨å¯¼å‡ºæœ€ä¼˜ç­–ç•¥ï¼š
$$\pi^*(y|x) = \frac{1}{Z(x)} \pi_{ref}(y|x) \exp\left(\frac{r(x,y)}{\beta}\right)$$

åè¿‡æ¥ï¼Œå¥–åŠ±å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š
$$r(x,y) = \beta \log \frac{\pi(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x)$$

**DPOæŸå¤±å‡½æ•°ï¼š**
```python
def dpo_loss(policy_model, ref_model, batch, beta=0.1):
    # è·å–ç­–ç•¥æ¨¡å‹çš„logæ¦‚ç‡
    policy_chosen_logprobs = policy_model.get_log_probs(
        batch['chosen_input_ids'],
        batch['chosen_attention_mask']
    )
    policy_rejected_logprobs = policy_model.get_log_probs(
        batch['rejected_input_ids'],
        batch['rejected_attention_mask']
    )
    
    # è·å–å‚è€ƒæ¨¡å‹çš„logæ¦‚ç‡
    with torch.no_grad():
        ref_chosen_logprobs = ref_model.get_log_probs(
            batch['chosen_input_ids'],
            batch['chosen_attention_mask']
        )
        ref_rejected_logprobs = ref_model.get_log_probs(
            batch['rejected_input_ids'],
            batch['rejected_attention_mask']
        )
    
    # è®¡ç®—logæ¯”ç‡
    chosen_logratios = policy_chosen_logprobs - ref_chosen_logprobs
    rejected_logratios = policy_rejected_logprobs - ref_rejected_logprobs
    
    # DPOæŸå¤±
    logits = beta * (chosen_logratios - rejected_logratios)
    loss = -F.logsigmoid(logits).mean()
    
    # è®¡ç®—å‡†ç¡®ç‡ï¼ˆç”¨äºç›‘æ§ï¼‰
    with torch.no_grad():
        accuracy = (logits > 0).float().mean()
    
    return loss, accuracy
```

**DPOè®­ç»ƒå™¨ï¼š**
```python
class DPOTrainer:
    def __init__(self, policy_model, ref_model, config):
        self.policy_model = policy_model
        self.ref_model = ref_model
        self.config = config
        
        # å†»ç»“å‚è€ƒæ¨¡å‹
        for param in self.ref_model.parameters():
            param.requires_grad = False
        
        # ä¼˜åŒ–å™¨åªé’ˆå¯¹ç­–ç•¥æ¨¡å‹
        self.optimizer = AdamW(
            self.policy_model.parameters(),
            lr=config.learning_rate
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=config.warmup_steps,
            num_training_steps=config.total_steps
        )
    
    def train_step(self, batch):
        self.policy_model.train()
        
        # è®¡ç®—æŸå¤±
        loss, accuracy = dpo_loss(
            self.policy_model,
            self.ref_model,
            batch,
            self.config.beta
        )
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(
            self.policy_model.parameters(),
            self.config.max_grad_norm
        )
        
        self.optimizer.step()
        self.scheduler.step()
        
        return {
            'loss': loss.item(),
            'accuracy': accuracy.item(),
            'lr': self.scheduler.get_last_lr()[0]
        }
    
    def evaluate(self, eval_dataset):
        self.policy_model.eval()
        total_loss = 0
        total_accuracy = 0
        
        with torch.no_grad():
            for batch in DataLoader(eval_dataset, batch_size=self.config.eval_batch_size):
                loss, accuracy = dpo_loss(
                    self.policy_model,
                    self.ref_model,
                    batch,
                    self.config.beta
                )
                total_loss += loss.item()
                total_accuracy += accuracy.item()
        
        return {
            'eval_loss': total_loss / len(eval_dataset),
            'eval_accuracy': total_accuracy / len(eval_dataset)
        }
```

### 4.4.2 èº«ä»½åå¥½ä¼˜åŒ–ï¼ˆIPOï¼‰

**IPOçš„åŠ¨æœºï¼š**
DPOåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½è¿‡æ‹Ÿåˆï¼ŒIPOé€šè¿‡ä¸åŒçš„æŸå¤±å‡½æ•°è®¾è®¡æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚

**IPOæŸå¤±å‡½æ•°ï¼š**
```python
def ipo_loss(policy_model, ref_model, batch, beta=0.1):
    # è·å–logæ¦‚ç‡ï¼ˆä¸DPOç›¸åŒï¼‰
    policy_chosen_logprobs = policy_model.get_log_probs(
        batch['chosen_input_ids'],
        batch['chosen_attention_mask']
    )
    policy_rejected_logprobs = policy_model.get_log_probs(
        batch['rejected_input_ids'],
        batch['rejected_attention_mask']
    )
    
    with torch.no_grad():
        ref_chosen_logprobs = ref_model.get_log_probs(
            batch['chosen_input_ids'],
            batch['chosen_attention_mask']
        )
        ref_rejected_logprobs = ref_model.get_log_probs(
            batch['rejected_input_ids'],
            batch['rejected_attention_mask']
        )
    
    # è®¡ç®—logæ¯”ç‡
    chosen_logratios = policy_chosen_logprobs - ref_chosen_logprobs
    rejected_logratios = policy_rejected_logprobs - ref_rejected_logprobs
    
    # IPOæŸå¤±ï¼ˆå¹³æ–¹æŸå¤±è€ŒélogisticæŸå¤±ï¼‰
    loss = ((chosen_logratios - rejected_logratios) - (1 / beta))**2
    
    return loss.mean()
```

### 4.4.3 ç®—æ³•å¯¹æ¯”åˆ†æ

**å®ç°ç»Ÿä¸€æ¥å£ï¼š**
```python
class AlignmentAlgorithm(ABC):
    """å¯¹é½ç®—æ³•çš„ç»Ÿä¸€æ¥å£"""
    
    @abstractmethod
    def compute_loss(self, batch):
        pass
    
    @abstractmethod
    def train_step(self, batch):
        pass
    
    @abstractmethod
    def get_required_models(self):
        """è¿”å›ç®—æ³•éœ€è¦çš„æ¨¡å‹"""
        pass

class PPOAlgorithm(AlignmentAlgorithm):
    def __init__(self, policy_model, ref_model, reward_model, config):
        self.policy_model = policy_model
        self.ref_model = ref_model
        self.reward_model = reward_model
        self.config = config
        
        # PPOç‰¹æœ‰çš„ç»„ä»¶
        self.value_model = ValueNetwork(policy_model.config.hidden_size)
        
    def compute_loss(self, batch):
        # PPOéœ€è¦å…ˆé‡‡æ ·ç”Ÿæˆ
        experiences = self.generate_experiences(batch['prompts'])
        
        # è®¡ç®—ä¼˜åŠ¿
        advantages = self.compute_advantages(experiences)
        
        # PPOæŸå¤±
        return self.ppo_loss(experiences, advantages)
    
    def get_required_models(self):
        return {
            'policy': self.policy_model,
            'reference': self.ref_model,
            'reward': self.reward_model,
            'value': self.value_model
        }

class DPOAlgorithm(AlignmentAlgorithm):
    def __init__(self, policy_model, ref_model, config):
        self.policy_model = policy_model
        self.ref_model = ref_model
        self.config = config
    
    def compute_loss(self, batch):
        return dpo_loss(
            self.policy_model,
            self.ref_model,
            batch,
            self.config.beta
        )[0]
    
    def get_required_models(self):
        return {
            'policy': self.policy_model,
            'reference': self.ref_model
        }
```

**æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼š**
```python
class AlignmentBenchmark:
    def __init__(self, algorithms, datasets, metrics):
        self.algorithms = algorithms
        self.datasets = datasets
        self.metrics = metrics
        
    def run_benchmark(self, num_steps=1000):
        results = defaultdict(dict)
        
        for algo_name, algorithm in self.algorithms.items():
            print(f"Benchmarking {algo_name}...")
            
            for dataset_name, dataset in self.datasets.items():
                # è®­ç»ƒ
                start_time = time.time()
                training_history = self.train_algorithm(
                    algorithm,
                    dataset,
                    num_steps
                )
                training_time = time.time() - start_time
                
                # è¯„ä¼°
                eval_results = self.evaluate_algorithm(
                    algorithm,
                    dataset.test_set
                )
                
                results[algo_name][dataset_name] = {
                    'training_history': training_history,
                    'training_time': training_time,
                    'eval_results': eval_results
                }
        
        return results
    
    def train_algorithm(self, algorithm, dataset, num_steps):
        history = defaultdict(list)
        dataloader = DataLoader(
            dataset.train_set,
            batch_size=32,
            shuffle=True
        )
        
        data_iter = iter(dataloader)
        for step in range(num_steps):
            try:
                batch = next(data_iter)
            except StopIteration:
                data_iter = iter(dataloader)
                batch = next(data_iter)
            
            metrics = algorithm.train_step(batch)
            
            for key, value in metrics.items():
                history[key].append(value)
        
        return history
    
    def evaluate_algorithm(self, algorithm, test_set):
        results = {}
        
        for metric_name, metric_fn in self.metrics.items():
            results[metric_name] = metric_fn(algorithm, test_set)
        
        return results
```

**ç®—æ³•ç‰¹æ€§å¯¹æ¯”ï¼š**

| ç‰¹æ€§ | PPO | DPO | IPO |
|------|-----|-----|-----|
| éœ€è¦å¥–åŠ±æ¨¡å‹ | âœ“ | âœ— | âœ— |
| éœ€è¦åœ¨çº¿é‡‡æ · | âœ“ | âœ— | âœ— |
| è®­ç»ƒç¨³å®šæ€§ | ä¸­ | é«˜ | é«˜ |
| è®¡ç®—æ•ˆç‡ | ä½ | é«˜ | é«˜ |
| ç†è®ºä¿è¯ | æœ‰ | æœ‰ | æœ‰ |
| è¶…å‚æ•°æ•æ„Ÿåº¦ | é«˜ | ä½ | ä½ |

#### ç»ƒä¹  4.4ï¼šå®ç°ç®—æ³•é€‰æ‹©å™¨
è®¾è®¡ä¸€ä¸ªç³»ç»Ÿï¼Œæ ¹æ®æ•°æ®ç‰¹æ€§å’Œèµ„æºçº¦æŸè‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„å¯¹é½ç®—æ³•ã€‚

<details>
<summary>æŸ¥çœ‹ç­”æ¡ˆ</summary>

**æ™ºèƒ½ç®—æ³•é€‰æ‹©å™¨å®ç°ï¼š**

```python
class AlignmentAlgorithmSelector:
    def __init__(self):
        self.feature_extractors = {
            'data': DataFeatureExtractor(),
            'model': ModelFeatureExtractor(),
            'resource': ResourceFeatureExtractor()
        }
        
        # å†³ç­–æ¨¡å‹ï¼ˆå¯ä»¥æ˜¯è§„åˆ™æˆ–å­¦ä¹ å¾—åˆ°çš„ï¼‰
        self.decision_model = self.load_decision_model()
        
        # ç®—æ³•é…ç½®æ¨¡æ¿
        self.algorithm_configs = {
            'ppo': {
                'pros': ['flexible', 'well_tested', 'good_for_complex_rewards'],
                'cons': ['computationally_expensive', 'requires_reward_model', 'complex_implementation'],
                'requirements': {
                    'gpu_memory': 'high',
                    'training_time': 'long',
                    'data_quality': 'medium'
                }
            },
            'dpo': {
                'pros': ['simple', 'efficient', 'no_reward_model'],
                'cons': ['less_flexible', 'potential_overfitting'],
                'requirements': {
                    'gpu_memory': 'medium',
                    'training_time': 'short',
                    'data_quality': 'high'
                }
            },
            'ipo': {
                'pros': ['robust', 'efficient', 'theoretically_motivated'],
                'cons': ['newer_method', 'less_tested'],
                'requirements': {
                    'gpu_memory': 'medium',
                    'training_time': 'short',
                    'data_quality': 'high'
                }
            }
        }
    
    def select_algorithm(self, context):
        """æ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©æœ€ä½³ç®—æ³•"""
        # 1. æå–ç‰¹å¾
        features = self.extract_features(context)
        
        # 2. è¯„åˆ†æ¯ä¸ªç®—æ³•
        scores = {}
        for algo_name in self.algorithm_configs:
            scores[algo_name] = self.score_algorithm(algo_name, features)
        
        # 3. é€‰æ‹©æœ€é«˜åˆ†çš„ç®—æ³•
        best_algorithm = max(scores, key=scores.get)
        
        # 4. ç”Ÿæˆæ¨èæŠ¥å‘Š
        recommendation = self.generate_recommendation(
            best_algorithm,
            scores,
            features
        )
        
        return best_algorithm, recommendation
    
    def extract_features(self, context):
        """ä»ä¸Šä¸‹æ–‡æå–ç›¸å…³ç‰¹å¾"""
        features = {}
        
        # æ•°æ®ç‰¹å¾
        features['data'] = self.feature_extractors['data'].extract(
            context['dataset']
        )
        
        # æ¨¡å‹ç‰¹å¾
        features['model'] = self.feature_extractors['model'].extract(
            context['model']
        )
        
        # èµ„æºçº¦æŸ
        features['resource'] = self.feature_extractors['resource'].extract(
            context['resources']
        )
        
        # ä»»åŠ¡éœ€æ±‚
        features['requirements'] = context.get('requirements', {})
        
        return features
    
    def score_algorithm(self, algo_name, features):
        """ä¸ºç®—æ³•æ‰“åˆ†"""
        score = 0
        algo_config = self.algorithm_configs[algo_name]
        
        # 1. èµ„æºåŒ¹é…åº¦
        resource_score = self.compute_resource_match(
            algo_config['requirements'],
            features['resource']
        )
        score += resource_score * 0.3
        
        # 2. æ•°æ®é€‚é…åº¦
        data_score = self.compute_data_match(
            algo_name,
            features['data']
        )
        score += data_score * 0.3
        
        # 3. ä»»åŠ¡éœ€æ±‚åŒ¹é…
        requirement_score = self.compute_requirement_match(
            algo_config,
            features['requirements']
        )
        score += requirement_score * 0.2
        
        # 4. æ¨¡å‹å…¼å®¹æ€§
        model_score = self.compute_model_compatibility(
            algo_name,
            features['model']
        )
        score += model_score * 0.2
        
        return score
    
    def compute_resource_match(self, algo_requirements, available_resources):
        """è®¡ç®—èµ„æºåŒ¹é…åº¦"""
        scores = []
        
        # GPUå†…å­˜
        if algo_requirements['gpu_memory'] == 'high':
            gpu_score = min(1.0, available_resources['gpu_memory'] / 40)  # 40GBä¸ºé«˜
        elif algo_requirements['gpu_memory'] == 'medium':
            gpu_score = min(1.0, available_resources['gpu_memory'] / 16)  # 16GBä¸ºä¸­
        else:
            gpu_score = 1.0
        scores.append(gpu_score)
        
        # è®­ç»ƒæ—¶é—´
        if algo_requirements['training_time'] == 'long':
            time_score = min(1.0, available_resources['max_training_hours'] / 168)  # ä¸€å‘¨
        elif algo_requirements['training_time'] == 'short':
            time_score = min(1.0, available_resources['max_training_hours'] / 24)  # ä¸€å¤©
        else:
            time_score = 1.0
        scores.append(time_score)
        
        return np.mean(scores)
    
    def compute_data_match(self, algo_name, data_features):
        """è®¡ç®—æ•°æ®é€‚é…åº¦"""
        if algo_name == 'ppo':
            # PPOå¯¹æ•°æ®è´¨é‡è¦æ±‚ç›¸å¯¹å®½æ¾
            quality_weight = 0.3
            quantity_weight = 0.7
        else:  # DPO/IPO
            # DPO/IPOéœ€è¦é«˜è´¨é‡çš„åå¥½æ•°æ®
            quality_weight = 0.7
            quantity_weight = 0.3
        
        quality_score = data_features['quality_score']
        quantity_score = min(1.0, data_features['num_comparisons'] / 50000)
        
        return quality_weight * quality_score + quantity_weight * quantity_score
    
    def compute_requirement_match(self, algo_config, requirements):
        """è®¡ç®—éœ€æ±‚åŒ¹é…åº¦"""
        score = 0
        matches = 0
        
        for req_key, req_value in requirements.items():
            if req_value in algo_config['pros']:
                score += 1
                matches += 1
            elif req_value in algo_config['cons']:
                score -= 0.5
                matches += 1
        
        if matches == 0:
            return 0.5  # ä¸­æ€§åˆ†æ•°
        
        return (score / matches + 1) / 2  # å½’ä¸€åŒ–åˆ°[0,1]
    
    def compute_model_compatibility(self, algo_name, model_features):
        """è®¡ç®—æ¨¡å‹å…¼å®¹æ€§"""
        # æ£€æŸ¥æ¨¡å‹å¤§å°
        model_size = model_features['num_parameters']
        
        if algo_name == 'ppo':
            # PPOéœ€è¦é¢å¤–çš„ä»·å€¼ç½‘ç»œï¼Œå¯¹å¤§æ¨¡å‹æ›´å›°éš¾
            if model_size > 10e9:  # 10B+
                size_score = 0.6
            else:
                size_score = 1.0
        else:
            # DPO/IPOå¯¹æ¨¡å‹å¤§å°æ›´å®½å®¹
            size_score = 1.0
        
        # æ£€æŸ¥æ¶æ„å…¼å®¹æ€§
        if model_features['architecture'] in ['gpt', 'llama', 'opt']:
            arch_score = 1.0
        else:
            arch_score = 0.8  # å…¶ä»–æ¶æ„å¯èƒ½éœ€è¦é€‚é…
        
        return (size_score + arch_score) / 2
    
    def generate_recommendation(self, best_algorithm, scores, features):
        """ç”Ÿæˆè¯¦ç»†çš„æ¨èæŠ¥å‘Š"""
        report = {
            'recommended_algorithm': best_algorithm,
            'confidence': scores[best_algorithm],
            'all_scores': scores,
            'reasoning': []
        }
        
        # æ·»åŠ æ¨ç†è¿‡ç¨‹
        report['reasoning'].append(
            f"Based on your resources: {features['resource']}"
        )
        report['reasoning'].append(
            f"Data characteristics: {features['data']}"
        )
        
        # ä¼˜åŠ¿è¯´æ˜
        algo_config = self.algorithm_configs[best_algorithm]
        report['advantages'] = algo_config['pros']
        report['disadvantages'] = algo_config['cons']
        
        # é…ç½®å»ºè®®
        report['suggested_config'] = self.suggest_hyperparameters(
            best_algorithm,
            features
        )
        
        # å¤‡é€‰æ–¹æ¡ˆ
        sorted_algos = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        if len(sorted_algos) > 1:
            report['alternative'] = {
                'algorithm': sorted_algos[1][0],
                'score': sorted_algos[1][1],
                'when_to_use': self.get_alternative_conditions(
                    sorted_algos[1][0],
                    best_algorithm
                )
            }
        
        return report
    
    def suggest_hyperparameters(self, algorithm, features):
        """åŸºäºç‰¹å¾æ¨èè¶…å‚æ•°"""
        if algorithm == 'ppo':
            config = {
                'learning_rate': 1e-6 if features['model']['num_parameters'] > 1e9 else 5e-6,
                'batch_size': min(32, features['resource']['gpu_memory'] // 2),
                'ppo_epochs': 4,
                'clip_epsilon': 0.2,
                'value_coef': 0.1,
                'kl_coef': 0.05 if features['data']['quality_score'] < 0.8 else 0.1
            }
        elif algorithm == 'dpo':
            config = {
                'learning_rate': 5e-7 if features['model']['num_parameters'] > 1e9 else 1e-6,
                'batch_size': min(64, features['resource']['gpu_memory'] // 1.5),
                'beta': 0.1 if features['data']['preference_strength'] > 0.7 else 0.2,
                'warmup_ratio': 0.1
            }
        else:  # IPO
            config = {
                'learning_rate': 5e-7 if features['model']['num_parameters'] > 1e9 else 1e-6,
                'batch_size': min(64, features['resource']['gpu_memory'] // 1.5),
                'beta': 0.1,
                'warmup_ratio': 0.1
            }
        
        return config
```

**ç‰¹å¾æå–å™¨å®ç°ï¼š**
```python
class DataFeatureExtractor:
    def extract(self, dataset):
        features = {}
        
        # æ•°æ®é‡
        features['num_comparisons'] = len(dataset)
        
        # è´¨é‡è¯„ä¼°
        features['quality_score'] = self.assess_quality(dataset)
        
        # åå¥½å¼ºåº¦åˆ†å¸ƒ
        margins = [item.get('margin', 1.0) for item in dataset]
        features['preference_strength'] = np.mean(margins)
        features['preference_variance'] = np.var(margins)
        
        # å¤šæ ·æ€§
        features['prompt_diversity'] = self.compute_diversity(
            [item['prompt'] for item in dataset]
        )
        
        # å“åº”é•¿åº¦åˆ†å¸ƒ
        response_lengths = [
            len(item['chosen'].split()) + len(item['rejected'].split())
            for item in dataset
        ]
        features['avg_response_length'] = np.mean(response_lengths)
        
        return features
    
    def assess_quality(self, dataset):
        """è¯„ä¼°æ•°æ®é›†è´¨é‡"""
        quality_factors = []
        
        # æ£€æŸ¥æ ‡æ³¨ä¸€è‡´æ€§ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ ‡æ³¨è€…ï¼‰
        if 'annotator_agreement' in dataset[0]:
            agreements = [item['annotator_agreement'] for item in dataset]
            quality_factors.append(np.mean(agreements))
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        completeness = sum(
            1 for item in dataset 
            if all(key in item for key in ['prompt', 'chosen', 'rejected'])
        ) / len(dataset)
        quality_factors.append(completeness)
        
        # æ£€æŸ¥åå¥½æ˜ç¡®æ€§
        if 'margin' in dataset[0]:
            margins = [item['margin'] for item in dataset]
            clarity = np.mean([1 if m > 0.3 else m/0.3 for m in margins])
            quality_factors.append(clarity)
        
        return np.mean(quality_factors) if quality_factors else 0.5

class ModelFeatureExtractor:
    def extract(self, model):
        features = {}
        
        # æ¨¡å‹è§„æ¨¡
        features['num_parameters'] = sum(p.numel() for p in model.parameters())
        
        # æ¶æ„ç±»å‹
        features['architecture'] = self.detect_architecture(model)
        
        # å†…å­˜å ç”¨
        features['memory_footprint'] = self.estimate_memory(model)
        
        # æ¨ç†é€Ÿåº¦
        features['inference_speed'] = self.benchmark_speed(model)
        
        return features
    
    def detect_architecture(self, model):
        """æ£€æµ‹æ¨¡å‹æ¶æ„ç±»å‹"""
        model_class = model.__class__.__name__.lower()
        
        if 'gpt' in model_class:
            return 'gpt'
        elif 'llama' in model_class:
            return 'llama'
        elif 'opt' in model_class:
            return 'opt'
        elif 't5' in model_class:
            return 't5'
        else:
            return 'unknown'

class ResourceFeatureExtractor:
    def extract(self, resources):
        features = {}
        
        # GPUèµ„æº
        features['gpu_memory'] = resources.get('gpu_memory_gb', 16)
        features['num_gpus'] = resources.get('num_gpus', 1)
        
        # æ—¶é—´çº¦æŸ
        features['max_training_hours'] = resources.get('max_hours', 24)
        
        # è®¡ç®—é¢„ç®—
        features['compute_budget'] = resources.get('compute_budget', 'medium')
        
        return features
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
# åˆå§‹åŒ–é€‰æ‹©å™¨
selector = AlignmentAlgorithmSelector()

# å®šä¹‰ä¸Šä¸‹æ–‡
context = {
    'dataset': preference_dataset,
    'model': language_model,
    'resources': {
        'gpu_memory_gb': 24,
        'num_gpus': 2,
        'max_hours': 48
    },
    'requirements': {
        'training_efficiency': True,
        'no_reward_model': True,
        'robust_to_overfitting': True
    }
}

# è·å–æ¨è
best_algo, recommendation = selector.select_algorithm(context)

print(f"Recommended algorithm: {best_algo}")
print(f"Confidence: {recommendation['confidence']:.2f}")
print(f"Reasoning: {recommendation['reasoning']}")
print(f"Suggested config: {recommendation['suggested_config']}")

# åŸºäºæ¨èåˆ›å»ºç®—æ³•
if best_algo == 'ppo':
    algorithm = PPOAlgorithm(
        policy_model,
        ref_model,
        reward_model,
        recommendation['suggested_config']
    )
elif best_algo == 'dpo':
    algorithm = DPOAlgorithm(
        policy_model,
        ref_model,
        recommendation['suggested_config']
    )
else:  # ipo
    algorithm = IPOAlgorithm(
        policy_model,
        ref_model,
        recommendation['suggested_config']
    )
```

</details>

### 4.4.4 æ··åˆæ–¹æ³•

**ç»“åˆä¸åŒç®—æ³•çš„ä¼˜åŠ¿ï¼š**
```python
class HybridAlignment:
    def __init__(self, policy_model, ref_model, reward_model, config):
        self.policy_model = policy_model
        self.ref_model = ref_model
        self.reward_model = reward_model
        self.config = config
        
        # é˜¶æ®µæ§åˆ¶
        self.current_stage = 'warmup'
        self.stage_steps = {
            'warmup': config.warmup_steps,
            'main': config.main_steps,
            'refinement': config.refinement_steps
        }
    
    def train_step(self, batch):
        if self.current_stage == 'warmup':
            # ä½¿ç”¨DPOå¿«é€Ÿå¯¹é½
            loss = self.dpo_step(batch)
        elif self.current_stage == 'main':
            # ä½¿ç”¨PPOç²¾ç»†è°ƒæ•´
            loss = self.ppo_step(batch)
        else:  # refinement
            # ä½¿ç”¨IPOç¨³å®šä¼˜åŒ–
            loss = self.ipo_step(batch)
        
        # æ›´æ–°é˜¶æ®µ
        self.update_stage()
        
        return loss
```

**âš¡ è®¾è®¡é€‰æ‹©ï¼š**
é€‰æ‹©å¯¹é½ç®—æ³•æ—¶çš„è€ƒè™‘å› ç´ ï¼š
- è®¡ç®—èµ„æºï¼šPPOæœ€æ˜‚è´µï¼ŒDPO/IPOæ›´é«˜æ•ˆ
- æ•°æ®è´¨é‡ï¼šDPO/IPOå¯¹æ•°æ®è´¨é‡è¦æ±‚æ›´é«˜
- çµæ´»æ€§ï¼šPPOå¯ä»¥ä½¿ç”¨ä»»æ„å¥–åŠ±å‡½æ•°
- å®ç°å¤æ‚åº¦ï¼šDPO/IPOæ›´ç®€å•

**ğŸ”¬ ç ”ç©¶çº¿ç´¢ï¼š**
- èƒ½å¦è®¾è®¡ç»“åˆåœ¨çº¿å’Œç¦»çº¿ä¼˜åŠ¿çš„æ–°ç®—æ³•ï¼Ÿ
- å¦‚ä½•è‡ªé€‚åº”é€‰æ‹©ä¸åŒé˜¶æ®µçš„ç®—æ³•ï¼Ÿ
- æ˜¯å¦å­˜åœ¨ç†è®ºæœ€ä¼˜çš„å¯¹é½ç®—æ³•ï¼Ÿ

---

[â† è¿”å›ç›®å½•](index.md) | [ä¸Šä¸€èŠ‚ï¼šå¥–åŠ±æ¨¡å‹çš„è®¾è®¡ä¸è®­ç»ƒ â†’](#section3) | [ä¸‹ä¸€èŠ‚ï¼šConstitutional AIä¸è‡ªæˆ‘æ”¹è¿› â†’](#section5)