# 第1章: Transformer架构深度剖析

Transformer架构自2017年提出以来，已成为现代大型语言模型的基石。本章将深入剖析Transformer的每个组件，不仅解释"是什么"和"怎么做"，更重要的是探讨"为什么"——每个设计选择背后的原理、权衡和替代方案。通过本章学习，你将建立对Transformer架构的深刻理解，为后续章节打下坚实基础。

## 本章内容

1. [注意力机制的数学本质](#section1) - 从first principles理解attention
2. [多头注意力与设计选择](#section2) - 为什么需要多头？头数如何选择？
3. [位置编码方案对比](#section3) - 正弦编码vs学习编码vs相对位置
4. [前馈网络与激活函数](#section4) - FFN的作用与激活函数演进
5. [层归一化与残差连接](#section5) - 稳定训练的关键技术
6. [Transformer变体与演进](#section6) - 架构改进的探索历程

---

## <a name="section1"></a>1.1 注意力机制的数学本质

注意力机制是Transformer的核心创新。让我们从最基本的数学原理开始，逐步理解其设计哲学。

### 1.1.1 从加权平均到注意力

注意力机制的本质是一种动态的加权平均。给定一个查询(query) q和一组键值对(key-value pairs) {(k₁,v₁), (k₂,v₂), ..., (kₙ,vₙ)}，注意力机制计算：

```
Attention(q, K, V) = Σᵢ α(q, kᵢ) · vᵢ
```

其中α(q, kᵢ)是注意力权重，满足Σᵢ α(q, kᵢ) = 1。

### 1.1.2 缩放点积注意力

Transformer采用的具体形式是缩放点积注意力(Scaled Dot-Product Attention)：

```
Attention(Q, K, V) = softmax(QK^T / √d_k)V
```

这里有几个关键设计选择：

**1. 为什么用点积？**
- 计算效率高：矩阵乘法可以高度优化
- 语义合理：点积度量向量相似度
- 可学习性：通过学习Q、K的表示空间来调整相似度计算

替代方案：
- 加性注意力：α(q, k) = v^T tanh(W_q q + W_k k)
- 乘性注意力的其他形式：如使用其他核函数
- 学习的相似度函数：通过神经网络计算相似度

这些方案在特定场景下可能更优，但计算效率通常较低。

**2. 为什么要缩放？**

当d_k较大时，点积的方差会随维度增长，导致softmax的梯度趋近于0。缩放因子1/√d_k保证了点积的方差稳定在1附近。

如果使用其他初始化方案（如改变Q、K的初始化方差），是否可以避免显式缩放？这是一个值得探索的方向。一些研究表明，通过精心设计的初始化和归一化，可以实现"自然缩放"的效果。

### 1.1.3 注意力的计算复杂度

标准注意力的计算复杂度为O(n²d)，其中n是序列长度，d是隐藏维度。这个二次复杂度是长序列处理的主要瓶颈。

<h4>练习 1.1：证明注意力计算复杂度</h4>
<p>证明自注意力机制的时间复杂度为O(n²d)，空间复杂度为O(n²)。分析哪些操作是瓶颈。</p>

<div class="hint">
提示：分别考虑QK^T的计算（O(n²d)）和softmax(·)V的计算（O(n²d)）。
</div>

<details class="answer-section">
<summary class="answer-toggle">查看答案</summary>
<div class="answer-content">

**时间复杂度分析：**

1. 计算QK^T：
   - Q的形状：[n, d]
   - K^T的形状：[d, n]
   - 矩阵乘法：O(n × d × n) = O(n²d)

2. Softmax操作：
   - 输入形状：[n, n]
   - 每行计算softmax：O(n)
   - 总共n行：O(n²)

3. 与V相乘：
   - Softmax输出形状：[n, n]
   - V的形状：[n, d]
   - 矩阵乘法：O(n × n × d) = O(n²d)

总时间复杂度：O(n²d) + O(n²) + O(n²d) = O(n²d)

**空间复杂度分析：**
- 存储注意力矩阵QK^T：O(n²)
- 这是主要的空间瓶颈

**瓶颈分析：**
- 当n >> d时（如长文本），n²项主导
- 当d >> n时（如短序列但模型很宽），计算瓶颈在矩阵乘法的d维度
- 实践中通常n更容易成为瓶颈，因此有了各种稀疏注意力的研究

</div>
</details>
</div>

### 1.1.4 注意力的几何解释

从几何角度看，注意力机制在做什么？

1. **投影空间**：Q和K被投影到同一个空间中进行相似度计算
2. **信息路由**：注意力权重决定了信息如何从不同位置流向当前位置
3. **动态感受野**：不同于CNN的固定感受野，注意力提供了动态的、内容相关的感受野

<div class="exercise">
<h4>练习 1.2：注意力模式可视化</h4>
<p>设计一个实验来可视化不同类型输入的注意力模式。思考：什么样的输入会产生局部注意力模式？什么样的会产生全局模式？</p>

<details class="answer-section">
<summary class="answer-toggle">查看答案</summary>
<div class="answer-content">

**实验设计：**

1. **局部注意力模式的输入：**
   - 重复模式：如"ABABAB..."
   - 局部依赖：如括号匹配"((()))"
   - 顺序任务：如排序、计数

2. **全局注意力模式的输入：**
   - 长距离依赖：如照应消解
   - 全局统计：如计算序列中某元素出现次数
   - 需要比较的任务：如找最大值

3. **可视化方法：**
   - 热力图：显示注意力权重矩阵
   - 连接图：显示超过阈值的注意力连接
   - 聚合统计：如注意力距离分布

4. **预期发现：**
   - 低层倾向于局部模式
   - 高层出现更多全局模式
   - 特定头可能专门化于特定模式

</div>
</details>
</div>

### 1.1.5 注意力的信息论视角

从信息论角度，注意力机制可以理解为一种信息瓶颈(Information Bottleneck)：

- **信息压缩**：将n个d维向量压缩为1个d维向量
- **相关性提取**：通过注意力权重保留最相关的信息
- **条件独立性**：假设给定注意力权重后，输出与原始输入条件独立

<div class="research-note">
如果放松softmax约束（如使用sparsemax或其他稀疏化方法），会如何影响信息传递？稀疏注意力是否能在保持性能的同时提供更好的可解释性？这是当前研究的热点方向。
</div>

### 1.1.6 自注意力vs交叉注意力

Transformer中使用了两种注意力：

1. **自注意力(Self-Attention)**：Q、K、V来自同一序列
   - 用途：建模序列内部依赖关系
   - 特点：需要因果掩码(causal mask)来防止信息泄露

2. **交叉注意力(Cross-Attention)**：Q来自一个序列，K、V来自另一个序列
   - 用途：encoder-decoder架构中的信息传递
   - 特点：不需要掩码，可以看到完整的源序列

<div class="alternative">
<strong>设计选择：</strong>
在某些架构中（如GPT），只使用自注意力。这简化了架构但限制了某些能力。在多模态模型中，交叉注意力变得更加重要，用于融合不同模态的信息。
</div>

<div class="exercise">
<h4>练习 1.3：因果掩码的必要性</h4>
<p>解释为什么自回归模型需要因果掩码。如果不使用会发生什么？设计一个实验来验证。</p>

<details class="answer-section">
<summary class="answer-toggle">查看答案</summary>
<div class="answer-content">

**因果掩码的必要性：**

1. **训练-推理不一致**：
   - 训练时：如果能看到未来信息，模型会"作弊"
   - 推理时：只能看到过去信息，导致分布偏移

2. **信息泄露的后果**：
   - 模型学会复制答案而非真正理解
   - 梯度捷径：直接从未来位置复制，而非学习预测

3. **实验设计：**
   ```
   任务：预测序列"A B C D E"
   
   无掩码训练：
   - 输入：[A B C D]
   - 目标：[B C D E]
   - 问题：预测B时能看到B本身！
   
   有掩码训练：
   - 预测B时只能看到A
   - 预测C时只能看到A、B
   - 正确的自回归行为
   ```

4. **验证实验：**
   - 训练两个模型：有/无因果掩码
   - 测试任务：简单序列延续
   - 预期结果：无掩码模型在训练集上loss极低，但推理时完全失败
