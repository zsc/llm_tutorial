# ç¬¬3ç« ï¼šå¾®è°ƒæŠ€æœ¯ä¸å¯¹é½æ–¹æ³•

é¢„è®­ç»ƒæ¨¡å‹å…·å¤‡äº†å¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä½†è¦å°†å…¶è½¬åŒ–ä¸ºå®ç”¨çš„AIåŠ©æ‰‹ï¼Œè¿˜éœ€è¦é€šè¿‡å¾®è°ƒæ¥é€‚åº”ç‰¹å®šä»»åŠ¡å’Œäººç±»åå¥½ã€‚æœ¬ç« æ·±å…¥æ¢è®¨ä»é¢„è®­ç»ƒåˆ°å®ç”¨ç³»ç»Ÿçš„å…³é”®æŠ€æœ¯æ¡¥æ¢ã€‚

## ç« èŠ‚ç›®å½•

1. [ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åŸºç¡€](#section1)
2. [å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯](#section2)  
3. [æŒ‡ä»¤éµå¾ªèƒ½åŠ›åŸ¹å…»](#section3)
4. [å¯¹é½æ–¹æ³•æ¦‚è§ˆ](#section4)
5. [æ•°æ®è´¨é‡ä¸å¤šæ ·æ€§](#section5)
6. [è¯„ä¼°ä¸è¿­ä»£æ”¹è¿›](#section6)

---

## <a name="section1"></a>3.1 ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åŸºç¡€

ç›‘ç£å¾®è°ƒæ˜¯å°†é¢„è®­ç»ƒæ¨¡å‹é€‚é…åˆ°ç‰¹å®šä»»åŠ¡çš„ç¬¬ä¸€æ­¥ã€‚è™½ç„¶æ¦‚å¿µç®€å•ï¼Œä½†ç»†èŠ‚å†³å®šæˆè´¥ã€‚

### 3.1.1 ä»é¢„è®­ç»ƒåˆ°å¾®è°ƒçš„èŒƒå¼è½¬å˜

**é¢„è®­ç»ƒ vs å¾®è°ƒçš„æœ¬è´¨åŒºåˆ«ï¼š**

| ç»´åº¦ | é¢„è®­ç»ƒ | å¾®è°ƒ |
|------|--------|------|
| ç›®æ ‡ | å­¦ä¹ é€šç”¨è¯­è¨€æ¨¡å¼ | å­¦ä¹ ç‰¹å®šä»»åŠ¡æ¨¡å¼ |
| æ•°æ®è§„æ¨¡ | TBçº§åˆ« | GBçº§åˆ« |
| æ•°æ®è´¨é‡ | å®¹å¿å™ªå£° | è¦æ±‚é«˜è´¨é‡ |
| å­¦ä¹ ç‡ | è¾ƒå¤§ï¼ˆ1e-4ï¼‰ | è¾ƒå°ï¼ˆ1e-5ï¼‰ |
| è®­ç»ƒæ—¶é•¿ | æ•°æœˆ | æ•°å¤© |

**å¾®è°ƒçš„æ•°å­¦è§†è§’ï¼š**
$$\theta_{fine} = \arg\min_{\theta} \mathcal{L}_{task}(\theta) + \lambda ||\theta - \theta_{pre}||^2$$

ç¬¬äºŒé¡¹æ˜¯éšå¼çš„æ­£åˆ™åŒ–ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚

### 3.1.2 å…¨å‚æ•°å¾®è°ƒæµç¨‹

**æ ‡å‡†æµç¨‹ï¼š**
```python
def supervised_finetuning(pretrained_model, dataset, config):
    model = load_pretrained(pretrained_model)
    
    # å…³é”®é…ç½®
    optimizer = AdamW(
        model.parameters(),
        lr=config.learning_rate,  # 1e-5 to 1e-6
        weight_decay=0.01
    )
    
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=config.warmup_ratio * total_steps,
        num_training_steps=total_steps
    )
    
    for epoch in range(config.num_epochs):  # é€šå¸¸3-5ä¸ªepoch
        for batch in dataloader:
            loss = compute_loss(model, batch)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(
                model.parameters(), 
                config.max_grad_norm
            )
            
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
```

**å…³é”®è¶…å‚æ•°é€‰æ‹©ï¼š**
1. **å­¦ä¹ ç‡**ï¼š
   - å¤ªå¤§â†’ç¾éš¾æ€§é—å¿˜
   - å¤ªå°â†’æ”¶æ•›æ…¢/æ¬ æ‹Ÿåˆ
   - ç»éªŒå€¼ï¼š1e-5åˆ°5e-6

2. **æ‰¹å¤§å°**ï¼š
   - å½±å“æ¢¯åº¦å™ªå£°
   - å—æ˜¾å­˜é™åˆ¶
   - gradient accumulationè¡¥æ•‘

3. **è®­ç»ƒè½®æ•°**ï¼š
   - è¿‡å°‘â†’æ¬ æ‹Ÿåˆ
   - è¿‡å¤šâ†’è¿‡æ‹Ÿåˆ
   - early stoppingç›‘æ§

### 3.1.3 ç¾éš¾æ€§é—å¿˜ä¸ç¼“è§£ç­–ç•¥

**é—®é¢˜è¡¨ç°ï¼š**
- åœ¨æ–°ä»»åŠ¡ä¸Šè¡¨ç°æå‡
- ä½†é€šç”¨èƒ½åŠ›æ€¥å‰§ä¸‹é™
- ç‰¹åˆ«æ˜¯å°‘è§ä»»åŠ¡/è¯­è¨€

**ç¼“è§£ç­–ç•¥ï¼š**

**1. æ··åˆè®­ç»ƒæ•°æ®**
```python
def create_mixed_dataset(task_data, general_data, mix_ratio=0.1):
    # ä¿ç•™éƒ¨åˆ†é¢„è®­ç»ƒæ•°æ®
    mixed_data = []
    for task_sample in task_data:
        mixed_data.append(task_sample)
        if random.random() < mix_ratio:
            mixed_data.append(random.choice(general_data))
    return mixed_data
```

**2. æ­£åˆ™åŒ–æŠ€æœ¯**
- L2æ­£åˆ™åŒ–
- Dropoutä¿æŒå¼€å¯
- çŸ¥è¯†è’¸é¦ä»åŸæ¨¡å‹

**3. æ¸è¿›å¼è§£å†»**
```python
def progressive_unfreezing(model, stage):
    # é€æ­¥è§£å†»æ›´å¤šå±‚
    if stage == 1:
        unfreeze_layers(model, ['lm_head'])
    elif stage == 2:
        unfreeze_layers(model, ['lm_head', 'transformer.h[-2:]'])
    elif stage == 3:
        unfreeze_layers(model, 'all')
```

### 3.1.4 ä»»åŠ¡ç‰¹å®šçš„é€‚é…æŠ€å·§

**åˆ†ç±»ä»»åŠ¡ï¼š**
- æ·»åŠ åˆ†ç±»å¤´
- ä½¿ç”¨pooled representation
- æ ‡ç­¾å¹³æ»‘ç¼“è§£è¿‡æ‹Ÿåˆ

**ç”Ÿæˆä»»åŠ¡ï¼š**
- ä¿æŒè‡ªå›å½’ç›®æ ‡
- è°ƒæ•´ç”Ÿæˆé•¿åº¦é™åˆ¶
- æ§åˆ¶é‡å¤æƒ©ç½š

**é—®ç­”ä»»åŠ¡ï¼š**
- è®¾è®¡åˆé€‚çš„promptæ ¼å¼
- åŒºåˆ†é—®é¢˜å’Œä¸Šä¸‹æ–‡
- ç­”æ¡ˆæŠ½å–vsç”Ÿæˆ

#### ç»ƒä¹  3.1ï¼šè®¾è®¡å¾®è°ƒå®éªŒ
ç»™å®šä¸€ä¸ªæƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼Œè®¾è®¡å®Œæ•´çš„å¾®è°ƒæ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€è®­ç»ƒé…ç½®å’Œè¯„ä¼°æŒ‡æ ‡ã€‚

<details>
<summary>æŸ¥çœ‹ç­”æ¡ˆ</summary>

**å®Œæ•´å¾®è°ƒæ–¹æ¡ˆï¼š**

1. **æ•°æ®é¢„å¤„ç†ï¼š**
   ```python
   def preprocess_sentiment_data(examples):
       # æ ‡å‡†åŒ–æ ‡ç­¾
       label_map = {'positive': 1, 'negative': 0, 'neutral': 2}
       
       # æ„é€ è¾“å…¥æ ¼å¼
       prompts = []
       for text, label in examples:
           prompt = f"Classify the sentiment: {text}\nSentiment:"
           prompts.append({
               'input': prompt,
               'target': label_map[label],
               'text_only': text  # ç”¨äºæ•°æ®å¢å¼º
           })
       
       return prompts
   ```

2. **æ•°æ®å¢å¼ºç­–ç•¥ï¼š**
   - åŒä¹‰è¯æ›¿æ¢ï¼ˆä¿æŒæƒ…æ„Ÿææ€§ï¼‰
   - å›è¯‘å¢å¼º
   - å¯¹æŠ—æ ·æœ¬ç”Ÿæˆ

3. **è®­ç»ƒé…ç½®ï¼š**
   ```python
   config = {
       'learning_rate': 2e-5,
       'warmup_ratio': 0.1,
       'batch_size': 32,
       'gradient_accumulation': 4,
       'num_epochs': 3,
       'weight_decay': 0.01,
       'max_grad_norm': 1.0,
       'eval_steps': 100,
       'save_steps': 500,
       'early_stopping_patience': 3
   }
   ```

4. **è¯„ä¼°æŒ‡æ ‡ï¼š**
   - å‡†ç¡®ç‡ï¼ˆä¸»è¦ï¼‰
   - F1åˆ†æ•°ï¼ˆç±»åˆ«å¹³è¡¡ï¼‰
   - æ··æ·†çŸ©é˜µï¼ˆé”™è¯¯åˆ†æï¼‰
   - æ¨ç†æ—¶é—´ï¼ˆå®ç”¨æ€§ï¼‰

5. **é˜²æ­¢è¿‡æ‹Ÿåˆï¼š**
   - éªŒè¯é›†ç›‘æ§
   - Dropout = 0.1
   - æ ‡ç­¾å¹³æ»‘ = 0.1
   - æ•°æ®å¢å¼º

6. **é”™è¯¯åˆ†æï¼š**
   ```python
   def error_analysis(model, test_data):
       errors = []
       for example in test_data:
           pred = model.predict(example['input'])
           if pred != example['target']:
               errors.append({
                   'text': example['text_only'],
                   'true': example['target'],
                   'pred': pred,
                   'confidence': model.get_confidence()
               })
       
       # æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„
       analyze_error_patterns(errors)
   ```

</details>

### 3.1.5 å¾®è°ƒçš„è®¡ç®—ä¼˜åŒ–

**æ··åˆç²¾åº¦è®­ç»ƒï¼š**
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    loss = model(input_ids, labels=labels).loss

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**æ¢¯åº¦ç´¯ç§¯ï¼š**
```python
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = compute_loss(model, batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**DeepSpeedé›†æˆï¼š**
- ZeRO-2ï¼šé€‚åˆå•èŠ‚ç‚¹å¤šå¡
- ZeRO-3ï¼šè·¨èŠ‚ç‚¹å¤§æ¨¡å‹
- CPUå¸è½½ï¼šæ˜¾å­˜ä¸è¶³æ—¶

### 3.1.6 å¾®è°ƒæ•ˆæœè¯Šæ–­

**è®­ç»ƒæ›²çº¿åˆ†æï¼š**
1. **Lossæ›²çº¿**ï¼š
   - å¹³æ»‘ä¸‹é™â†’æ­£å¸¸
   - éœ‡è¡â†’å­¦ä¹ ç‡è¿‡å¤§
   - å¹³å°â†’å­¦ä¹ ç‡è¿‡å°

2. **éªŒè¯æŒ‡æ ‡**ï¼š
   - æŒç»­ä¸Šå‡â†’æ­£å¸¸
   - æ—©æœŸä¸‹é™â†’è¿‡æ‹Ÿåˆ
   - ä¸å˜â†’æ¬ æ‹Ÿåˆ

**æ¢¯åº¦å¥åº·æ£€æŸ¥ï¼š**
```python
def check_gradient_health(model):
    total_norm = 0
    param_count = 0
    
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
            param_count += 1
    
    total_norm = total_norm ** 0.5
    avg_norm = total_norm / param_count
    
    # å¥åº·èŒƒå›´ï¼š0.01 - 10
    return avg_norm
```

**ğŸ”¬ ç ”ç©¶çº¿ç´¢ï¼š**
- å¦‚ä½•è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è¶…å‚æ•°ï¼Ÿ
- èƒ½å¦é¢„æµ‹éœ€è¦çš„è®­ç»ƒæ•°æ®é‡ï¼Ÿ
- å¦‚ä½•é‡åŒ–ç¾éš¾æ€§é—å¿˜ç¨‹åº¦ï¼Ÿ

---

## <a name="section2"></a>3.2 å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯

å½“æ¨¡å‹è§„æ¨¡è¾¾åˆ°æ•°åäº¿å‚æ•°æ—¶ï¼Œå…¨å‚æ•°å¾®è°ƒå˜å¾—ä¸åˆ‡å®é™…ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯åº”è¿è€Œç”Ÿã€‚

### 3.2.1 LoRAï¼šä½ç§©é€‚åº”

**æ ¸å¿ƒæ€æƒ³ï¼š**
å¾®è°ƒç­‰ä»·äºå­¦ä¹ ä¸€ä¸ªä½ç§©çš„å‚æ•°æ›´æ–°ã€‚

**æ•°å­¦åŸç†ï¼š**
$$W_{fine} = W_{pre} + \Delta W = W_{pre} + BA$$

å…¶ä¸­ $B \in \mathbb{R}^{d \times r}$ï¼Œ$A \in \mathbb{R}^{r \times k}$ï¼Œ$r \ll \min(d, k)$ã€‚

**å®ç°ç»†èŠ‚ï¼š**
```python
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=16, alpha=16):
        super().__init__()
        self.W = nn.Linear(in_features, out_features, bias=False)
        # å†»ç»“é¢„è®­ç»ƒæƒé‡
        self.W.weight.requires_grad = False
        
        # ä½ç§©åˆ†è§£
        self.lora_A = nn.Linear(in_features, rank, bias=False)
        self.lora_B = nn.Linear(rank, out_features, bias=False)
        
        # ç¼©æ”¾å› å­
        self.scaling = alpha / rank
        
        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B.weight)
    
    def forward(self, x):
        # åŸå§‹å‰å‘ + ä½ç§©æ›´æ–°
        return self.W(x) + self.lora_B(self.lora_A(x)) * self.scaling
```

**è¶…å‚æ•°é€‰æ‹©ï¼š**
- rankï¼š4-64ï¼Œè¶Šå¤§è¡¨è¾¾åŠ›è¶Šå¼º
- alphaï¼šé€šå¸¸ç­‰äºrank
- ç›®æ ‡æ¨¡å—ï¼šQã€Væœ€é‡è¦ï¼ŒKã€Oæ¬¡ä¹‹

**ä¼˜åŠ¿åˆ†æï¼š**
1. å‚æ•°é‡ï¼šå‡å°‘10000å€
2. æ˜¾å­˜ï¼šåªéœ€å­˜å‚¨rä¸ªå‚æ•°
3. åˆ‡æ¢ä»»åŠ¡ï¼šæ›´æ¢LoRAæƒé‡å³å¯
4. è®­ç»ƒé€Ÿåº¦ï¼š3-10å€åŠ é€Ÿ

### 3.2.2 QLoRAï¼šé‡åŒ–LoRA

**åˆ›æ–°ç‚¹ï¼š**
åŸºç¡€æ¨¡å‹é‡åŒ–åˆ°4-bitï¼Œåªæœ‰LoRAéƒ¨åˆ†ä½¿ç”¨å…¨ç²¾åº¦ã€‚

**å…³é”®æŠ€æœ¯ï¼š**
1. **NF4é‡åŒ–**ï¼š
   - ä¸“ä¸ºæ­£æ€åˆ†å¸ƒè®¾è®¡
   - ä¿¡æ¯æŸå¤±æœ€å°

2. **åŒé‡é‡åŒ–**ï¼š
   - é‡åŒ–æƒé‡
   - é‡åŒ–é‡åŒ–å¸¸æ•°

3. **åˆ†é¡µä¼˜åŒ–å™¨**ï¼š
   - è‡ªåŠ¨CPUå¸è½½
   - é¿å…OOM

**å®ç°è¦ç‚¹ï¼š**
```python
def prepare_model_for_qlora(model):
    # 1. é‡åŒ–åŸºç¡€æ¨¡å‹
    model = load_in_4bit(
        model,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    )
    
    # 2. å‡†å¤‡è®­ç»ƒ
    model = prepare_model_for_kbit_training(model)
    
    # 3. æ·»åŠ LoRA
    config = LoraConfig(
        r=64,
        lora_alpha=64,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, config)
    return model
```

**å†…å­˜è®¡ç®—ï¼š**
```python
def estimate_qlora_memory(model_size_B, lora_rank):
    # åŸºç¡€æ¨¡å‹ï¼ˆ4-bitï¼‰
    base_memory = model_size_B * 0.5  # GB
    
    # LoRAå‚æ•°ï¼ˆ16-bitï¼‰
    lora_params = model_size_B * 0.001 * (lora_rank / 16)
    lora_memory = lora_params * 2  # GB
    
    # ä¼˜åŒ–å™¨çŠ¶æ€
    optimizer_memory = lora_memory * 4
    
    # æ¿€æ´»å€¼ï¼ˆæ‰¹å¤§å°ç›¸å…³ï¼‰
    activation_memory = 2  # GB (ä¼°è®¡)
    
    total = base_memory + lora_memory + optimizer_memory + activation_memory
    return total
```

### 3.2.3 Prefix Tuningä¸Prompt Tuning

**Prefix Tuningï¼š**
åœ¨æ¯å±‚æ³¨å…¥å¯å­¦ä¹ çš„å‰ç¼€å‘é‡ã€‚

```python
class PrefixTuning(nn.Module):
    def __init__(self, num_layers, num_heads, head_dim, prefix_len=10):
        super().__init__()
        self.prefix_len = prefix_len
        
        # æ¯å±‚çš„å‰ç¼€embeddings
        self.prefix_embeddings = nn.ModuleList([
            nn.Embedding(prefix_len, num_heads * head_dim * 2)  # Kå’ŒV
            for _ in range(num_layers)
        ])
        
    def forward(self, layer_idx, key_values):
        prefix = self.prefix_embeddings[layer_idx](
            torch.arange(self.prefix_len, device=key_values.device)
        )
        
        # åˆ†ç¦»Kå’ŒV
        prefix_k, prefix_v = prefix.chunk(2, dim=-1)
        
        # æ‹¼æ¥åˆ°åŸå§‹Kã€V
        key = torch.cat([prefix_k, key_values[0]], dim=1)
        value = torch.cat([prefix_v, key_values[1]], dim=1)
        
        return key, value
```

**Prompt Tuningï¼š**
åªåœ¨è¾“å…¥å±‚æ·»åŠ å¯å­¦ä¹ å‘é‡ã€‚

**å¯¹æ¯”åˆ†æï¼š**
| æ–¹æ³• | å‚æ•°é‡ | è¡¨è¾¾åŠ› | è®­ç»ƒéš¾åº¦ |
|------|--------|--------|----------|
| Prefix | ä¸­ç­‰ | å¼º | è¾ƒéš¾ |
| Prompt | æœ€å°‘ | å¼± | å®¹æ˜“ |
| LoRA | è¾ƒå°‘ | æœ€å¼º | ä¸­ç­‰ |

### 3.2.4 Adapterï¼šç“¶é¢ˆå±‚æ–¹æ³•

**æ¶æ„è®¾è®¡ï¼š**
```python
class Adapter(nn.Module):
    def __init__(self, d_model, bottleneck_dim=64):
        super().__init__()
        self.down_project = nn.Linear(d_model, bottleneck_dim)
        self.activation = nn.GELU()
        self.up_project = nn.Linear(bottleneck_dim, d_model)
        
    def forward(self, x):
        residual = x
        x = self.down_project(x)
        x = self.activation(x)
        x = self.up_project(x)
        return x + residual  # æ®‹å·®è¿æ¥
```

**æ’å…¥ä½ç½®ï¼š**
- FFNä¹‹å
- è‡ªæ³¨æ„åŠ›ä¹‹å
- æˆ–ä¸¤è€…éƒ½åŠ 

**ä¸LoRAå¯¹æ¯”ï¼š**
- Adapterï¼šä¸²è¡Œè®¡ç®—ï¼Œæœ‰å»¶è¿Ÿ
- LoRAï¼šå¹¶è¡Œè®¡ç®—ï¼Œæ— é¢å¤–å»¶è¿Ÿ
- ä½†Adapteræ›´ç¨³å®š

#### ç»ƒä¹  3.2ï¼šè®¾è®¡æ··åˆPEFTç­–ç•¥
ç»“åˆå¤šç§PEFTæŠ€æœ¯ï¼Œè®¾è®¡ä¸€ä¸ªæ—¢é«˜æ•ˆåˆå¼ºå¤§çš„å¾®è°ƒæ–¹æ¡ˆã€‚

<details>
<summary>æŸ¥çœ‹ç­”æ¡ˆ</summary>

**æ··åˆPEFTæ–¹æ¡ˆè®¾è®¡ï¼š**

1. **æŠ€æœ¯ç»„åˆï¼š**
   - LoRAï¼šç”¨äºæ³¨æ„åŠ›å±‚ï¼ˆè¡¨è¾¾åŠ›ï¼‰
   - Adapterï¼šç”¨äºFFNå±‚ï¼ˆç¨³å®šæ€§ï¼‰
   - Prompt Tuningï¼šä»»åŠ¡ç‰¹å®šå‰ç¼€

2. **æ¶æ„å®ç°ï¼š**
   ```python
   class HybridPEFT(nn.Module):
       def __init__(self, base_model, config):
           super().__init__()
           self.base_model = base_model
           
           # LoRA for attention
           self.lora_modules = nn.ModuleDict()
           for name, module in base_model.named_modules():
               if 'attention' in name and isinstance(module, nn.Linear):
                   self.lora_modules[name] = LoRALinear(
                       module.in_features,
                       module.out_features,
                       rank=config.lora_rank
                   )
           
           # Adapters for FFN
           self.adapters = nn.ModuleList([
               Adapter(config.d_model, config.adapter_dim)
               for _ in range(config.num_layers)
           ])
           
           # Task-specific prompts
           self.task_prompts = nn.ModuleDict({
               'classification': nn.Embedding(10, config.d_model),
               'generation': nn.Embedding(10, config.d_model),
               'qa': nn.Embedding(10, config.d_model)
           })
   ```

3. **è®­ç»ƒç­–ç•¥ï¼š**
   - ç¬¬ä¸€é˜¶æ®µï¼šåªè®­ç»ƒpromptsï¼ˆå¿«é€Ÿé€‚åº”ï¼‰
   - ç¬¬äºŒé˜¶æ®µï¼šè§£å†»LoRAï¼ˆç²¾ç»†è°ƒæ•´ï¼‰
   - ç¬¬ä¸‰é˜¶æ®µï¼šå…¨éƒ¨PEFTå‚æ•°ï¼ˆæœ€ç»ˆä¼˜åŒ–ï¼‰

4. **å‚æ•°åˆ†é…ï¼š**
   ```python
   def allocate_parameters(total_budget, model_size):
       # ç»éªŒåˆ†é…
       lora_ratio = 0.6
       adapter_ratio = 0.3
       prompt_ratio = 0.1
       
       lora_rank = int(total_budget * lora_ratio / (model_size * 0.01))
       adapter_dim = int(total_budget * adapter_ratio / (model_size * 0.02))
       prompt_len = int(total_budget * prompt_ratio / model_size)
       
       return {
           'lora_rank': min(64, lora_rank),
           'adapter_dim': min(128, adapter_dim),
           'prompt_len': min(20, prompt_len)
       }
   ```

5. **åŠ¨æ€é€‰æ‹©ï¼š**
   ```python
   def select_peft_method(task_type, data_size, model_size):
       if data_size < 1000:
           return 'prompt_tuning'  # æ•°æ®å°‘
       elif model_size > 10e9 and data_size < 10000:
           return 'qlora'  # å¤§æ¨¡å‹å°æ•°æ®
       elif task_type in ['classification', 'ner']:
           return 'adapter'  # ç¨³å®šæ€§é‡è¦
       else:
           return 'hybrid'  # ç»¼åˆæœ€ä¼˜
   ```

</details>

### 3.2.5 PEFTæŠ€æœ¯çš„ç»Ÿä¸€è§†è§’

**æ•°å­¦ç»Ÿä¸€ï¼š**
æ‰€æœ‰PEFTæ–¹æ³•éƒ½å¯ä»¥çœ‹ä½œåœ¨å‚æ•°ç©ºé—´æ–½åŠ çº¦æŸï¼š
$$\theta_{fine} = \theta_{pre} + P\delta$$

å…¶ä¸­Pæ˜¯æŠ•å½±çŸ©é˜µï¼š
- LoRAï¼šä½ç§©æŠ•å½±
- Adapterï¼šç“¶é¢ˆæŠ•å½±
- Prefixï¼šä½ç½®æŠ•å½±

**é€‰æ‹©æŒ‡å—ï¼š**
```python
def recommend_peft_method(requirements):
    if requirements['memory_critical']:
        return 'qlora'
    elif requirements['inference_speed_critical']:
        return 'lora'  # æ— é¢å¤–å»¶è¿Ÿ
    elif requirements['stability_critical']:
        return 'adapter'
    elif requirements['few_shot']:
        return 'prompt_tuning'
    else:
        return 'lora'  # é»˜è®¤é€‰æ‹©
```

### 3.2.6 PEFTçš„æœªæ¥æ–¹å‘

**1. è‡ªåŠ¨åŒ–PEFTï¼š**
- è‡ªåŠ¨é€‰æ‹©ç§©
- è‡ªåŠ¨é€‰æ‹©æ¨¡å—
- ç¥ç»æ¶æ„æœç´¢

**2. ä»»åŠ¡é—´è¿ç§»ï¼š**
- LoRAç»„åˆ
- çŸ¥è¯†è’¸é¦
- æŒç»­å­¦ä¹ 

**3. ç†è®ºç†è§£ï¼š**
- ä¸ºä»€ä¹ˆä½ç§©æœ‰æ•ˆï¼Ÿ
- æœ€ä¼˜ç§©å¦‚ä½•ç¡®å®šï¼Ÿ
- ä¸åŒå±‚çš„é‡è¦æ€§ï¼Ÿ

**âš¡ è®¾è®¡é€‰æ‹©ï¼š**
é€‰æ‹©PEFTæ–¹æ³•æ—¶è€ƒè™‘ï¼š
- å‚æ•°é¢„ç®—
- æ¨ç†å»¶è¿Ÿè¦æ±‚
- ä»»åŠ¡ç±»å‹
- æ•°æ®è§„æ¨¡

**ğŸ”¬ ç ”ç©¶çº¿ç´¢ï¼š**
- PEFTæ–¹æ³•çš„ç†è®ºæœ€ä¼˜æ€§ï¼Ÿ
- å¦‚ä½•ç»„åˆå¤šä¸ªLoRAå®ç°æ–°èƒ½åŠ›ï¼Ÿ
- æ˜¯å¦å­˜åœ¨é€šç”¨çš„PEFTæ¶æ„ï¼Ÿ

---

[â† è¿”å›ç›®å½•](index.md) | [ä¸Šä¸€èŠ‚ï¼šç›‘ç£å¾®è°ƒåŸºç¡€ â†’](#section1) | [ä¸‹ä¸€èŠ‚ï¼šæŒ‡ä»¤éµå¾ªèƒ½åŠ›åŸ¹å…» â†’](#section3)